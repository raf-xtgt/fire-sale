{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d98860-ec15-4f82-a6dd-2c1a10d66a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install pandas\n",
    "!pip install faker\n",
    "!pip install beautifulsoup4 rake-nltk\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d377940b-2c35-4bbb-bbe5-0cba2fd47a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "972557a2-990d-4fa3-aebe-3c20549a51bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from appwrite.client import Client\n",
    "from appwrite.services.databases import Databases\n",
    "from appwrite.id import ID\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95900b0c-bb8b-499b-867e-45ccab788760",
   "metadata": {},
   "source": [
    "##### Library for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65fcfb09-1ca9-47fe-b779-8516c694058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/rafxtgt/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0706ef06-bfdf-4d33-bef0-0e617edd7a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<appwrite.client.Client at 0x7f7feffd9290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client()\n",
    "client.set_endpoint(os.getenv('APPWRITE_API_ENDPOINT'))\n",
    "client.set_project(os.getenv('APPWRITE_PROJECT'))\n",
    "client.set_key(os.getenv('APPWRITE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d86d695-e9b6-43b7-89df-954fce230152",
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = Databases(client)\n",
    "database_name = 'fire-sale-db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2831de2-1a61-42ff-9795-de21ab379d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'fire-sale-db' already exists with ID: 6812c6490009447d68d4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try to get the database to see if it exists\n",
    "    database_list = databases.list()\n",
    "    existing_db = next((db for db in database_list['databases'] if db['name'] == database_name), None)\n",
    "    \n",
    "    if existing_db:\n",
    "        fireSaleDb = existing_db\n",
    "        print(f\"Database '{database_name}' already exists with ID: {existing_db['$id']}\")\n",
    "    else:\n",
    "        # Create the database if it doesn't exist\n",
    "        fireSaleDb = databases.create(\n",
    "            database_id=ID.unique(),\n",
    "            name=database_name\n",
    "        )\n",
    "        print(f\"Created new database '{database_name}' with ID: {fireSaleDb['$id']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing database: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014d4de8-34b7-42ee-ba34-93535efe210f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$id': '6812c6490009447d68d4',\n",
       " 'name': 'fire-sale-db',\n",
       " '$createdAt': '2025-05-01T00:54:33.779+00:00',\n",
       " '$updatedAt': '2025-05-01T00:54:33.779+00:00',\n",
       " 'enabled': True,\n",
       " 'policies': [],\n",
       " 'archives': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fireSaleDb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a77aa8c-41cc-45fd-adc4-99920e0d4fea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### User Profile Collectin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a23c2a7-22e3-4539-a282-144c6aef18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "userProfileCollection = None\n",
    "\n",
    "def prepare_user_profile_collection():\n",
    "  global userProfileCollection\n",
    "  try:\n",
    "    collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "    for collection in collections['collections']:\n",
    "        if collection['name'] == 'user-profile':\n",
    "            userProfileCollection = collection\n",
    "            print(\"User profile collection already exists\")\n",
    "            return\n",
    "  except Exception as e:\n",
    "    print(f\"Error checking for existing collection: {e}\")\n",
    "      \n",
    "    \n",
    "  userProfileCollection = databases.create_collection(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=ID.unique(),\n",
    "    name='user-profile'\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='user_id',\n",
    "    size=255,\n",
    "    required=True\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='name',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='email',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='label',\n",
    "    size=255,\n",
    "    required=True\n",
    "  )\n",
    "    \n",
    "  databases.create_integer_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='age',\n",
    "    required=False,\n",
    "    min=18,\n",
    "    max=150\n",
    "  )\n",
    "    \n",
    "  databases.create_datetime_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='signup_date',\n",
    "    required=True\n",
    "  )\n",
    "  print(\"User profile collection created successfully !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca394faa-f840-4e68-a184-6d699fd40af9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Social Media Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "853e36e4-733e-482b-b551-5f2179a8704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "socialMediaCollection = None\n",
    "\n",
    "def prepare_social_media_collection():\n",
    "    global socialMediaCollection\n",
    "\n",
    "    try:\n",
    "        collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "        for collection in collections['collections']:\n",
    "            if collection['name'] == 'social-media':\n",
    "                socialMediaCollection = collection\n",
    "                print(\"Social media collection already exists\")\n",
    "                return\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking for existing collection: {e}\")\n",
    "    \n",
    "    # If collection doesn't exist, create it\n",
    "    socialMediaCollection = databases.create_collection(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=ID.unique(),\n",
    "        name='social-media'\n",
    "    )\n",
    "\n",
    "    # Create all the attributes\n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='guid',\n",
    "        size=255,\n",
    "        required=True\n",
    "    )\n",
    "        \n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='user_id',\n",
    "        size=255,\n",
    "        required=False\n",
    "    )\n",
    "\n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='platform',\n",
    "        size=255,\n",
    "        required=False\n",
    "    )\n",
    "\n",
    "    databases.create_integer_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='post_count',\n",
    "        required=False,\n",
    "        min=0\n",
    "    )  \n",
    "\n",
    "    databases.create_integer_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='like_count',\n",
    "        required=False,\n",
    "        min=0\n",
    "    )  \n",
    "\n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='groups',\n",
    "        required=False, \n",
    "        size=131072\n",
    "    )    \n",
    "        \n",
    "    databases.create_integer_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='follower_count',\n",
    "        required=False,\n",
    "        min=0\n",
    "    ) \n",
    "        \n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='top_interests',\n",
    "        required=False, \n",
    "        size=131072\n",
    "    )  \n",
    "        \n",
    "    databases.create_datetime_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='last_active',\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    print(\"Created new social media collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae12a8d-eea1-4334-8923-0a32b10c07cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Communication Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7701e348-dd3f-42f9-9626-64aedc6819ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "communicationCollection = None\n",
    "\n",
    "def prepare_communication_collection():\n",
    "  global communicationCollection\n",
    "  try:\n",
    "    collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "    for collection in collections['collections']:\n",
    "        if collection['name'] == 'communication':\n",
    "            communicationCollection = collection\n",
    "            print(\"Communication collection already exists\")\n",
    "            return\n",
    "  except Exception as e:\n",
    "    print(f\"Error checking for existing collection: {e}\")\n",
    "    \n",
    "\n",
    "  communicationCollection = databases.create_collection(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=ID.unique(),\n",
    "    name='communication'\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='guid',\n",
    "    size=255,\n",
    "    required=True\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='user_id',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='medium',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='contacted_user_id',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_integer_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='interaction_count',\n",
    "    required=False,\n",
    "    min=0\n",
    "  )  \n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='topics',\n",
    "    required=False, \n",
    "    size=131072\n",
    "  )    \n",
    "    \n",
    "  databases.create_datetime_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='last_contact_date',\n",
    "    required=False\n",
    "  )\n",
    "  print(\"Communication collection created successfully !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4bac8-a3b6-49d4-86bc-1a5ff28e3c40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Location Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb51ff7a-5a62-456d-a889-faeae1509230",
   "metadata": {},
   "outputs": [],
   "source": [
    "locationCollection = None\n",
    "\n",
    "def prepare_location_collection():\n",
    "  global locationCollection\n",
    "  try:\n",
    "    collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "    for collection in collections['collections']:\n",
    "        if collection['name'] == 'location':\n",
    "            locationCollection = collection\n",
    "            print(\"Location collection already exists\")\n",
    "            return\n",
    "  except Exception as e:\n",
    "    print(f\"Error checking for existing collection: {e}\")\n",
    "\n",
    "    \n",
    "  locationCollection = databases.create_collection(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=ID.unique(),\n",
    "    name='location'\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='guid',\n",
    "    size=255,\n",
    "    required=True\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='user_id',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='location_name',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "    \n",
    "  databases.create_float_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='latitude',\n",
    "    required=False\n",
    "  )\n",
    "    \n",
    "  databases.create_float_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='longitude',\n",
    "    required=False\n",
    "  )\n",
    "   \n",
    "  databases.create_datetime_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='check_in_time',\n",
    "    required=False\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='companions',\n",
    "    required=False, \n",
    "    size=131072\n",
    "  )\n",
    "  print(\"Location collection created successfully !\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe623188-538e-4a6c-b31b-e4a9caa573f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Behavioral Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67ea6321-0864-4e75-a0a6-0a1c602dbc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviorMetadataCollection = None\n",
    "\n",
    "def prepare_behaviour_metadata_collection():\n",
    "  global behaviorMetadataCollection\n",
    "  try:\n",
    "    collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "    for collection in collections['collections']:\n",
    "        if collection['name'] == 'behavioral-metadata':\n",
    "            behaviorMetadataCollection = collection\n",
    "            print(\"Behavioral metadata collection already exists\")\n",
    "            return\n",
    "  except Exception as e:\n",
    "    print(f\"Error checking for existing collection: {e}\")\n",
    "\n",
    "  behaviorMetadataCollection = databases.create_collection(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=ID.unique(),\n",
    "    name='behavioral-metadata'\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='guid',\n",
    "    size=255,\n",
    "    required=True\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='user_id',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='device_type',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='active_hours',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "      \n",
    "  databases.create_float_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='average_daily_screen_time',\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='preferred_app_categories',\n",
    "    required=False, \n",
    "    size=131072\n",
    "  )\n",
    "  print(\"Behavioral metadata collection created successfully !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7537065-93e7-4189-a22c-cb7ec37310c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### InferredProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae43e5ab-1365-4d46-9090-7f52281f4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferredProfileCollection = None\n",
    "\n",
    "def prepare_inferred_profile_collection():\n",
    "  global inferredProfileCollection\n",
    "  try:\n",
    "    collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "    for collection in collections['collections']:\n",
    "        if collection['name'] == 'inferred-profile':\n",
    "            inferredProfileCollection = collection\n",
    "            print(\"Inferred profile collection already exists\")\n",
    "            return\n",
    "  except Exception as e:\n",
    "    print(f\"Error checking for existing collection: {e}\")\n",
    "\n",
    "  inferredProfileCollection = databases.create_collection(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=ID.unique(),\n",
    "    name='inferred-profile'\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=inferredProfileCollection['$id'],\n",
    "    key='user_id',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_float_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=inferredProfileCollection['$id'],\n",
    "    key='influence_score',\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "    \n",
    "  databases.create_float_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=inferredProfileCollection['$id'],\n",
    "    key='trust_score',\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=inferredProfileCollection['$id'],\n",
    "    key='hidden_roles',\n",
    "    required=False, \n",
    "    size=131072\n",
    "  )  \n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=inferredProfileCollection['$id'],\n",
    "    key='hidden_communities',\n",
    "    required=False, \n",
    "    size=131072\n",
    "  )  \n",
    "  print(\"Inferred profile collection created successfully !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86dd8b8-785c-49d3-8b9e-c773d28d34c2",
   "metadata": {},
   "source": [
    "#### Create the database collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a4b575f-e12a-40e2-8675-9fafb310cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_string_attribute(attribute_name: str, size: int = 255, required: bool = False):\n",
    "    global socialMediaCollection\n",
    "    \n",
    "    if not socialMediaCollection:\n",
    "        raise Exception(\"Social media collection not initialized. Call prepare_social_media_collection() first.\")\n",
    "    \n",
    "    # Check if attribute already exists\n",
    "    existing_attributes = databases.list_attributes(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id']\n",
    "    )['attributes']\n",
    "    \n",
    "    if any(attr['key'] == attribute_name for attr in existing_attributes):\n",
    "        print(f\"Attribute '{attribute_name}' already exists\")\n",
    "        return\n",
    "    \n",
    "    # Create new string attribute\n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key=attribute_name,\n",
    "        size=size,\n",
    "        required=required\n",
    "    )\n",
    "    print(f\"Added new attribute '{attribute_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5e9c364-97fb-40a5-9445-dc473cb544c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new attribute 'work_exp'\n"
     ]
    }
   ],
   "source": [
    "add_new_string_attribute(\"work_exp\", size=131072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "576b7338-6899-489c-9bc7-d59a03f9caa5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User profile collection already exists\n",
      "Social media collection already exists\n",
      "Communication collection created successfully !\n",
      "Location collection already exists\n",
      "Behavioral metadata collection already exists\n",
      "Inferred profile collection already exists\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  prepare_user_profile_collection()\n",
    "  prepare_social_media_collection()\n",
    "  prepare_communication_collection()\n",
    "  prepare_location_collection()\n",
    "  prepare_behaviour_metadata_collection()\n",
    "  prepare_inferred_profile_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdf01434-be37-4eff-923e-b956b62d92d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = databases.delete(\n",
    "    database_id = '680fc5380017320a8568'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241fa69-fdfb-4729-8a7c-5f391f4771b1",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c427dbf-8f7e-4094-8d08-ca4a77e564f3",
   "metadata": {},
   "source": [
    "#### Create synthetic user profile data csv from the pseudo file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0036ebb-ae3e-4858-b862-0768ed1450ca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              user_guid            name  \\\n",
      "0  6c9799d2-dac3-4098-8fd3-1d0f7ae138bb  Jennifer Blair   \n",
      "1  186648a5-0093-46a1-8932-b696aeb8ed09   Alfred Cooper   \n",
      "2  1a3965d4-ccfb-4a8f-827a-8585a7ad1dc1     Debra Cohen   \n",
      "3  c39101b7-5518-4105-9072-0ba3a1abb1ca   Helen Holland   \n",
      "4  16741755-a8b9-4f5e-883b-4372fa0795bd   Monica Hansen   \n",
      "\n",
      "                          email label  user_age  \n",
      "0            troy75@example.org  NODE        26  \n",
      "1          pamela72@example.com  NODE        26  \n",
      "2      villamichael@example.org  NODE        26  \n",
      "3  hendersonstephen@example.com  NODE        26  \n",
      "4           mclarke@example.org  NODE        26  \n",
      "New CSV created with columns: ['user_guid', 'name', 'email', 'label', 'user_age']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "import uuid\n",
    "\n",
    "# Initialize Faker generator\n",
    "fake = Faker()\n",
    "Faker.seed(42)  # For reproducible results\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('dataset/pseudo_facebook.csv')\n",
    "\n",
    "# Calculate current year and user age\n",
    "current_year = datetime.now().year\n",
    "df['user_age'] = current_year - df['dob_year']\n",
    "df['label'] = 'NODE'\n",
    "\n",
    "# Generate unique synthetic data\n",
    "num_rows = len(df)\n",
    "unique_names = set()\n",
    "unique_emails = set()\n",
    "\n",
    "while len(unique_names) < num_rows:\n",
    "    unique_names.add(fake.unique.name())\n",
    "\n",
    "while len(unique_emails) < num_rows:\n",
    "    unique_emails.add(fake.unique.email())\n",
    "\n",
    "# Add the synthetic columns\n",
    "df['name'] = list(unique_names)\n",
    "df['email'] = list(unique_emails)\n",
    "df['user_guid'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "\n",
    "# Verify uniqueness\n",
    "# print(f\"Unique names generated: {len(df['name'].unique()) == len(df)}\")\n",
    "# print(f\"Unique emails generated: {len(df['email'].unique()) == len(df)}\")\n",
    "\n",
    "# Display the DataFrame with new age column\n",
    "\n",
    "selected_columns = ['user_guid', 'name', 'email', 'label', 'user_age']  \n",
    "new_df = df[selected_columns]\n",
    "print(new_df.head())\n",
    "new_df.to_csv('dataset/synthetic_user_profile_data.csv', index=False)\n",
    "\n",
    "print(f\"New CSV created with columns: {selected_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d5d64-a835-4c52-ab09-c95b26d9ba52",
   "metadata": {},
   "source": [
    "#### Create a smaller file for user interaction from facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e19875f-78f5-4d17-af9c-d51c15abfdbf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'html', 'political', 'not_political', 'title', 'message', 'thumbnail', 'created_at', 'updated_at', 'lang', 'images', 'impressions', 'political_probability', 'targeting', 'suppressed', 'targets', 'advertiser', 'entities', 'page', 'lower_page', 'targetings', 'paid_for_by', 'targetedness', 'listbuilding_fundraising_proba']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.csv' with your actual file path\n",
    "file_path = 'dataset/fbpac-ads-en-US.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "headers = df.columns.tolist()\n",
    "\n",
    "# Display the first 5 rows with headers\n",
    "df.head()\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ada8b86-223a-45aa-b60e-f0affecf2a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV created with columns: ['title', 'message', 'advertiser', 'entities', 'page']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read original CSV\n",
    "file_path = 'dataset/fbpac-ads-en-US.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select columns you want to keep (adjust names as needed)\n",
    "selected_columns = ['title', 'message', 'advertiser', 'entities', 'page']  \n",
    "\n",
    "# Create new DataFrame with only selected columns\n",
    "new_df = df[selected_columns]\n",
    "\n",
    "# Save to new CSV\n",
    "new_df.to_csv('dataset/compressed_ad_data.csv', index=False)\n",
    "\n",
    "print(f\"New CSV created with columns: {selected_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350d421-21bc-4f36-85c8-03d95135de42",
   "metadata": {},
   "source": [
    "#### Add the consumer_type attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7fea941b-5f09-4050-8f16-a2370db3d829",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AppwriteException",
     "evalue": "Attribute with the requested key already exists. Attribute keys must be unique, try again with a different key.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/work/fire-sale/data/FireSaleVenv/lib/python3.11/site-packages/appwrite/client.py:118\u001b[39m, in \u001b[36mClient.call\u001b[39m\u001b[34m(self, method, path, headers, params, response_type)\u001b[39m\n\u001b[32m    107\u001b[39m response = requests.request(  \u001b[38;5;66;03m# call method dynamically https://stackoverflow.com/a/4246075/2299554\u001b[39;00m\n\u001b[32m    108\u001b[39m     method=method,\n\u001b[32m    109\u001b[39m     url=\u001b[38;5;28mself\u001b[39m._endpoint + path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    115\u001b[39m     allow_redirects=\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m response_type == \u001b[33m'\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    116\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m warnings = response.headers.get(\u001b[33m'\u001b[39m\u001b[33mx-appwrite-warning\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/work/fire-sale/data/FireSaleVenv/lib/python3.11/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 409 Client Error: Conflict for url: https://fra.cloud.appwrite.io/v1/databases/6812c6490009447d68d4/collections/6812c6db000500ec4de7/attributes/string",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAppwriteException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdatabases\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_string_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfireSaleDb\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43muserProfileCollection\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconsumer_type\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/work/fire-sale/data/FireSaleVenv/lib/python3.11/site-packages/appwrite/services/databases.py:1404\u001b[39m, in \u001b[36mDatabases.create_string_attribute\u001b[39m\u001b[34m(self, database_id, collection_id, key, size, required, default, array, encrypt)\u001b[39m\n\u001b[32m   1401\u001b[39m api_params[\u001b[33m'\u001b[39m\u001b[33marray\u001b[39m\u001b[33m'\u001b[39m] = array\n\u001b[32m   1402\u001b[39m api_params[\u001b[33m'\u001b[39m\u001b[33mencrypt\u001b[39m\u001b[33m'\u001b[39m] = encrypt\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent-type\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/work/fire-sale/data/FireSaleVenv/lib/python3.11/site-packages/appwrite/client.py:138\u001b[39m, in \u001b[36mClient.call\u001b[39m\u001b[34m(self, method, path, headers, params, response_type)\u001b[39m\n\u001b[32m    136\u001b[39m content_type = response.headers[\u001b[33m'\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m content_type.startswith(\u001b[33m'\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AppwriteException(response.json()[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m], response.status_code, response.json().get(\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m), response.text)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AppwriteException(response.text, response.status_code, \u001b[38;5;28;01mNone\u001b[39;00m, response.text)\n",
      "\u001b[31mAppwriteException\u001b[39m: Attribute with the requested key already exists. Attribute keys must be unique, try again with a different key."
     ]
    }
   ],
   "source": [
    "databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=userProfileCollection['$id'],\n",
    "        key='consumer_type',\n",
    "        size=100,\n",
    "        required=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec3ad2-6e25-4761-8d7d-e78f7b2393ea",
   "metadata": {},
   "source": [
    "#### Update consumer type to 'INDIVIDUAL_CONSUMER' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4122f1c1-beae-4f8c-a18c-90ce2f4dd927",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute may already exist: Attribute with the requested key already exists. Attribute keys must be unique, try again with a different key.\n",
      "Fetched 500/500 documents...\n",
      "\n",
      "Successfully retrieved 500 documents\n",
      "Updated 100 documents...\n",
      "Updated 200 documents...\n",
      "Updated 300 documents...\n",
      "Updated 400 documents...\n",
      "Updated 500 documents...\n",
      "\n",
      "Successfully updated 500/500 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from appwrite.query import Query\n",
    "\n",
    "def get_all_documents():\n",
    "    \"\"\"Retrieve all documents from a collection with pagination\"\"\"\n",
    "    all_documents = []\n",
    "    offset = 0\n",
    "    limit = 500  # Number of documents per request\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get documents with pagination using Query\n",
    "            result = databases.list_documents(\n",
    "                database_id=fireSaleDb['$id'],\n",
    "                collection_id=userProfileCollection['$id'],\n",
    "                queries=[\n",
    "                    Query.limit(limit),\n",
    "                    Query.offset(offset)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            if not result['documents']:\n",
    "                break  # No more documents\n",
    "                \n",
    "            all_documents.extend(result['documents'])\n",
    "            total_documents = result['total']\n",
    "            offset += len(result['documents'])\n",
    "            \n",
    "            print(f\"Fetched {len(all_documents)}/{total_documents} documents...\")\n",
    "            \n",
    "            # Exit if we've got all documents\n",
    "            if len(all_documents) >= total_documents:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching documents: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nSuccessfully retrieved {len(all_documents)} documents\")\n",
    "    return all_documents\n",
    "\n",
    "def update_all_consumer_types():\n",
    "    # First ensure the attribute exists\n",
    "    try:\n",
    "        databases.create_string_attribute(\n",
    "            database_id=fireSaleDb['$id'],\n",
    "            collection_id=userProfileCollection['$id'],\n",
    "            key='consumer_type',\n",
    "            size=50,\n",
    "            required=False\n",
    "        )\n",
    "        print(\"Created consumer_type attribute\")\n",
    "    except Exception as e:\n",
    "        print(f\"Attribute may already exist: {str(e)}\")\n",
    "\n",
    "    # Get all documents\n",
    "    all_documents = get_all_documents()\n",
    "    \n",
    "    # Update all documents\n",
    "    success_count = 0\n",
    "    for doc in all_documents:\n",
    "        try:\n",
    "            databases.update_document(\n",
    "                database_id=fireSaleDb['$id'],\n",
    "                collection_id=userProfileCollection['$id'],\n",
    "                document_id=doc['$id'],\n",
    "                data={'consumer_type': 'INDIVIDUAL_CONSUMER'}\n",
    "            )\n",
    "            success_count += 1\n",
    "            if success_count % 100 == 0:\n",
    "                print(f\"Updated {success_count} documents...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating document {doc['$id']}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully updated {success_count}/{len(all_documents)} documents\")\n",
    "    return success_count\n",
    "\n",
    "# Execute the update\n",
    "update_all_consumer_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545c97e-783f-4bd2-971a-2237cd72787e",
   "metadata": {},
   "source": [
    "#### Twitter interaction data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2c95f9b-32ca-4c69-9710-2b882a476715",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  label\n",
      "0  is upset that he can't update his Facebook by ...      0\n",
      "1  @Kenichan I dived many times for the ball. Man...      0\n",
      "2    my whole body feels itchy and like its on fire       0\n",
      "3  @nationwideclass no, it's not behaving at all....      0\n",
      "4                      @Kwesidei not the whole crew       0\n",
      "Total number of rows: 1599999\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read original CSV\n",
    "file_path_ = 'dataset/twitter_dataset_1.csv'\n",
    "\n",
    "# Try different encodings\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='latin1')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Rename columns\n",
    "new_columns = ['label', 'user_id', 'post_date', 'type', 'user_name', 'tweet']\n",
    "if len(df.columns) == len(new_columns):\n",
    "    df.columns = new_columns  # Rename all columns at once\n",
    "else:\n",
    "    print(\"Warning: Column count does not match. Keeping original column names.\")\n",
    "\n",
    "selected_columns = ['tweet', 'label']  \n",
    "tweet_df_1 = df[selected_columns]\n",
    "print(tweet_df_1.head())\n",
    "print(\"Total number of rows:\", tweet_df_1.shape[0])\n",
    "\n",
    "# # Display results\n",
    "# print(\"\\nFirst 5 rows:\")\n",
    "# print(df.head())\n",
    "\n",
    "# print(\"\\nColumn headers:\")\n",
    "# print(df.columns.tolist())\n",
    "\n",
    "# print(\"\\nNumber of columns:\", len(df.columns))\n",
    "# print(\"Total number of rows:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b34f485-33ee-4251-808e-455631fc76d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  label\n",
      "0  just had a real good moment. i missssssssss hi...      0\n",
      "1         is reading manga  http://plurk.com/p/mzp1e      0\n",
      "2  @comeagainjen http://twitpic.com/2y2lx - http:...      0\n",
      "3  @lapcat Need to send 'em to my accountant tomo...      0\n",
      "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder      0\n",
      "Total number of rows: 10314\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read original CSV\n",
    "file_path_d1 = 'dataset/twitter_dataset_2.csv'\n",
    "selected_columns = ['tweet', 'label']  \n",
    "\n",
    "df1 = pd.read_csv(file_path_d1)\n",
    "tweet_df_2 = df1[selected_columns]\n",
    "\n",
    "# print(df1.head())\n",
    "print(tweet_df_2.head())\n",
    "print(\"Total number of rows:\", tweet_df_2.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbc20330-d89c-4abc-94b0-1d739e612eab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (1610313, 2)\n",
      "CSV file saved as 'combined_tweets.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenate the two DataFrames vertically (row-wise)\n",
    "combined_df = pd.concat([tweet_df_2, tweet_df_1], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv('dataset/combined_tweets.csv', index=False)  # index=False avoids saving an extra index column\n",
    "\n",
    "print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "print(\"CSV file saved as 'combined_tweets.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873b710-83cc-49ba-b13f-26a1c3b4759f",
   "metadata": {},
   "source": [
    "#### LinkedIn User profile and company data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f23cbd4-1ae2-48dc-b814-f10840885e5a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    timestamp                            id  \\\n",
      "0  2023-01-10            catherinemcilkenny   \n",
      "1  2022-12-17           margot-bon-51a04624   \n",
      "2  2023-05-17            mike-dean-8509a193   \n",
      "3  2022-05-29  giovanna-panarella-99a0a4167   \n",
      "4  2022-12-06         steve-latimer-3364327   \n",
      "\n",
      "                                     name                       city  \\\n",
      "0  Catherine Fitzpatrick (McIlkenny), B.A                     Canada   \n",
      "1                              Margot Bon  The Randstad, Netherlands   \n",
      "2                               Mike Dean    England, United Kingdom   \n",
      "3                      Giovanna Panarella  Avellino, Campania, Italy   \n",
      "4                           Steve Latimer            Ontario, Canada   \n",
      "\n",
      "  country_code region     current_company:company_id  \\\n",
      "0           CA    NaN                            NaN   \n",
      "1           NL     EU               gemeente-utrecht   \n",
      "2           UK    NaN                   network-rail   \n",
      "3           IT     EU                            NaN   \n",
      "4           CA    NaN  mid-range-computer-group-inc.   \n",
      "\n",
      "            current_company:name  \\\n",
      "0                            NaN   \n",
      "1               Gemeente Utrecht   \n",
      "2                   Network Rail   \n",
      "3                      Freelance   \n",
      "4  Mid-Range Computer Group Inc.   \n",
      "\n",
      "                                            position  following  ...  \\\n",
      "0  Snr Business Analyst at Emploi et Développemen...        NaN  ...   \n",
      "1  Communicatieadviseur Corporate & Strategie Gem...        NaN  ...   \n",
      "2               Network Data Manager at Network Rail        NaN  ...   \n",
      "3                             Architetto (Freelance)      500.0  ...   \n",
      "4  Senior Account Executive at Mid-Range Computer...        NaN  ...   \n",
      "\n",
      "                                  people_also_viewed  \\\n",
      "0  [{\"profile_link\":\"https://ca.linkedin.com/in/l...   \n",
      "1  [{\"profile_link\":\"https://nl.linkedin.com/in/j...   \n",
      "2  [{\"profile_link\":\"https://uk.linkedin.com/in/g...   \n",
      "3  [{\"profile_link\":\"https://it.linkedin.com/in/e...   \n",
      "4  [{\"profile_link\":\"https://ca.linkedin.com/in/d...   \n",
      "\n",
      "             educations_details  \\\n",
      "0    Queen's University Belfast   \n",
      "1                           NaN   \n",
      "2          Brighton Polytechnic   \n",
      "3        Università di Camerino   \n",
      "4  St. Michael's College School   \n",
      "\n",
      "                                           education  \\\n",
      "0  [{\"degree\":\"Bachelor of Arts (B.A.) Honours\",\"...   \n",
      "1  [{\"degree\":\"Scrum en Agile werken\",\"end_year\":...   \n",
      "2  [{\"degree\":\"2:2\",\"end_year\":\"1991\",\"field\":\"El...   \n",
      "3  [{\"degree\":\"“Corso di aggiornamento profession...   \n",
      "4  [{\"degree\":\"\",\"end_year\":\"1978\",\"field\":\"\",\"me...   \n",
      "\n",
      "                                              avatar  \\\n",
      "0  https://media.licdn.com/dms/image/C4E03AQEcz_j...   \n",
      "1  https://static.licdn.com/sc/h/244xhbkr7g40x6bs...   \n",
      "2  https://media.licdn.com/dms/image/C4D03AQHLj-Z...   \n",
      "3  https://media-exp1.licdn.com/dms/image/C4D03AQ...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                           languages  \\\n",
      "0                                                NaN   \n",
      "1  [{\"subtitle\":\"Professional working proficiency...   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                      certifications  \\\n",
      "0                                                NaN   \n",
      "1  [{\"meta\":\"Issued Jun 2013\",\"subtitle\":\"Van der...   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4  [{\"meta\":\"Issued Jan 2022 See credential\",\"sub...   \n",
      "\n",
      "                                     recommendations recommendations_count  \\\n",
      "0                                                NaN                   NaN   \n",
      "1  Menno H. Poort “Ik werk al jaren prettig met M...                   2.0   \n",
      "2                                                NaN                   NaN   \n",
      "3                                                NaN                   NaN   \n",
      "4  Blake Reeves “If I was a customer, I would wan...                   1.0   \n",
      "\n",
      "                                volunteer_experience сourses  \n",
      "0                                                NaN     NaN  \n",
      "1  [{\"cause\":\"\",\"duration\":\"Sep 2010 Jul 2020 9 y...     NaN  \n",
      "2                                                NaN     NaN  \n",
      "3  [{\"cause\":\"Arts and Culture\",\"duration\":\"Jan 2...     NaN  \n",
      "4                                                NaN     NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "Total number of rows: 1000\n",
      "\n",
      "Column headers:\n",
      "['timestamp', 'id', 'name', 'city', 'country_code', 'region', 'current_company:company_id', 'current_company:name', 'position', 'following', 'about', 'posts', 'groups', 'current_company', 'experience', 'url', 'people_also_viewed', 'educations_details', 'education', 'avatar', 'languages', 'certifications', 'recommendations', 'recommendations_count', 'volunteer_experience', 'сourses']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read original CSV\n",
    "file_path_d1 = 'dataset/LinkedIn_people_profiles_datasets.csv'\n",
    "\n",
    "df1 = pd.read_csv(file_path_d1)\n",
    "\n",
    "# print(df1.head())\n",
    "print(df1.head())\n",
    "print(\"Total number of rows:\", df1.shape[0])\n",
    "print(\"\\nColumn headers:\")\n",
    "print(df1.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b8956e-7ee2-4f82-adfd-85b261d381dd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            current_company_name  \\\n",
      "0                            NaN   \n",
      "1               Gemeente Utrecht   \n",
      "2                   Network Rail   \n",
      "3                      Freelance   \n",
      "4  Mid-Range Computer Group Inc.   \n",
      "\n",
      "                                            position  \\\n",
      "0  Snr Business Analyst at Emploi et Développemen...   \n",
      "1  Communicatieadviseur Corporate & Strategie Gem...   \n",
      "2               Network Data Manager at Network Rail   \n",
      "3                             Architetto (Freelance)   \n",
      "4  Senior Account Executive at Mid-Range Computer...   \n",
      "\n",
      "                                               about  \\\n",
      "0                                                NaN   \n",
      "1  Allround Marketing & Communicatie Adviseur met...   \n",
      "2  Experienced Data Manager with a demonstrated h...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                               posts  \\\n",
      "0  [{\"attribution\":\"Liked by Catherine Fitzpatric...   \n",
      "1  [{\"attribution\":\"Liked by Margot Bon\",\"img\":\"h...   \n",
      "2                                                NaN   \n",
      "3  [{\"attribution\":\"Liked by Giovanna Panarella\",...   \n",
      "4  [{\"attribution\":\"Liked by Steve Latimer\",\"link...   \n",
      "\n",
      "                                              groups  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3  [{\"img\":null,\"subtitle\":\"-\",\"title\":\"Sustainab...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                          experience  \\\n",
      "0                                                NaN   \n",
      "1  [{\"company\":\"Gemeente Utrecht\",\"company_id\":\"g...   \n",
      "2  [{\"company\":\"Network Rail\",\"company_id\":\"netwo...   \n",
      "3  [{\"company\":\"Freelance\",\"company_id\":null,\"loc...   \n",
      "4  [{\"company\":\"Mid-Range Computer Group Inc.\",\"c...   \n",
      "\n",
      "             educations_details  \n",
      "0    Queen's University Belfast  \n",
      "1                           NaN  \n",
      "2          Brighton Polytechnic  \n",
      "3        Università di Camerino  \n",
      "4  St. Michael's College School  \n"
     ]
    }
   ],
   "source": [
    "df1['current_company_name'] = df1['current_company:name']\n",
    "selected_columns = ['current_company_name', 'position', 'about', 'posts', 'groups', 'experience', 'educations_details']\n",
    "linkedIn_df = df1[selected_columns]\n",
    "print(linkedIn_df.head())\n",
    "linkedIn_df.to_csv('dataset/linkedin_user_profile.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be41fd7-87c2-4620-8d80-93e0d0c7fb6e",
   "metadata": {},
   "source": [
    "#### LinkedIn company profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a1ee0c7-70b3-493e-a0ed-df0f5e833504",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 1000\n",
      "\n",
      "Column headers:\n",
      "['timestamp', 'id', 'name', 'country_code', 'locations', 'formatted_locations', 'followers', 'employees_in_linkedin', 'about', 'specialties', 'company_size', 'organization_type', 'industries', 'website', 'crunchbase_url', 'founded', 'company_id', 'employees', 'headquarters', 'image', 'logo', 'similar', 'sphere', 'url', 'type', 'updates', 'slogan', 'affiliated', 'funding', 'stock_info', 'investors']\n",
      "                                        name  \\\n",
      "0  Be Nijs * Business- & Concept Development   \n",
      "1             Texas Deaf Chamber of Commerce   \n",
      "2                                CellPraxis®   \n",
      "3                        DIAMOND TOOLS PLANT   \n",
      "4                            Thieves Kitchen   \n",
      "\n",
      "                           industries  \\\n",
      "0    Business Consulting and Services   \n",
      "1      Civic and Social Organizations   \n",
      "2              Biotechnology Research   \n",
      "3  Industrial Machinery Manufacturing   \n",
      "4                    Media Production   \n",
      "\n",
      "                                         specialties  \\\n",
      "0  Expansie bedrijfsactiviteiten, Marketing strat...   \n",
      "1  Business Development, Mentoring, Networking, A...   \n",
      "2  Saúde, Ciências da vida, Terapia celular, Medi...   \n",
      "3                                                NaN   \n",
      "4  On Air Promos, Idents, Commercials, B2B Promos...   \n",
      "\n",
      "                                               about organization_type  \n",
      "0  Voor Startups, ondernemers en bedrijven die he...        Self-Owned  \n",
      "1  MISSION STATEMENT: “The mission of this Chambe...         Nonprofit  \n",
      "2  Fundada em 2008, a Cellpraxis® é uma empresa d...    Privately Held  \n",
      "3  Мы применяем самые передовые технологии для пр...       Partnership  \n",
      "4  Thieves Kitchen is a multi-award-winning produ...    Privately Held  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read original CSV\n",
    "file_path_d1 = 'dataset/linkedin_company_profile.csv'\n",
    "\n",
    "df1 = pd.read_csv(file_path_d1)\n",
    "\n",
    "# print(df1.head())\n",
    "# print(df1.head())\n",
    "print(\"Total number of rows:\", df1.shape[0])\n",
    "print(\"\\nColumn headers:\")\n",
    "print(df1.columns.tolist())\n",
    "selected_columns = ['name', 'industries', 'specialties', 'about', 'organization_type']\n",
    "linkedIn_comp_df = df1[selected_columns]\n",
    "print(linkedIn_comp_df.head())\n",
    "linkedIn_comp_df.to_csv('dataset/linkedin_comp_profile.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98920e8a-6001-4949-aab3-0b814355143e",
   "metadata": {},
   "source": [
    "### Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85c3c08b-d537-4c07-bcf1-2a507efc5f4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              user_guid            name  \\\n",
      "0  6c9799d2-dac3-4098-8fd3-1d0f7ae138bb  Jennifer Blair   \n",
      "1  186648a5-0093-46a1-8932-b696aeb8ed09   Alfred Cooper   \n",
      "2  1a3965d4-ccfb-4a8f-827a-8585a7ad1dc1     Debra Cohen   \n",
      "3  c39101b7-5518-4105-9072-0ba3a1abb1ca   Helen Holland   \n",
      "4  16741755-a8b9-4f5e-883b-4372fa0795bd   Monica Hansen   \n",
      "\n",
      "                          email label  user_age  \n",
      "0            troy75@example.org  NODE        26  \n",
      "1          pamela72@example.com  NODE        26  \n",
      "2      villamichael@example.org  NODE        26  \n",
      "3  hendersonstephen@example.com  NODE        26  \n",
      "4           mclarke@example.org  NODE        26  \n",
      "\n",
      "Column headers:\n",
      "['user_guid', 'name', 'email', 'label', 'user_age']\n",
      "Total number of rows: 99003\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "user_profile_df = pd.read_csv('dataset/synthetic_user_profile_data.csv')\n",
    "print(user_profile_df.head())\n",
    "print(\"\\nColumn headers:\")\n",
    "print(user_profile_df.columns.tolist())\n",
    "print(\"Total number of rows:\", user_profile_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972dd1e-a70f-48b9-814d-8ce73680f09e",
   "metadata": {},
   "source": [
    "#### Push first 500 rows into user profile dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0fa22c4-308c-4788-a200-dc226325d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def upload_user_profiles(df, limit=500):\n",
    "    # Take the first 500 rows\n",
    "    subset = df.head(limit)\n",
    "    \n",
    "    # Prepare the collection if not already prepared\n",
    "    if userProfileCollection is None:\n",
    "        prepare_user_profile_collection()\n",
    "    \n",
    "    # Initialize counters\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Iterate through each row and create document\n",
    "    for index, row in subset.iterrows():\n",
    "        try:\n",
    "            # Map the CSV columns to our collection attributes\n",
    "            document_data = {\n",
    "                'user_id': row['user_guid'],\n",
    "                'name': row['name'],\n",
    "                'email': row['email'],\n",
    "                'label': row['label'],\n",
    "                'age': int(row['user_age']) if pd.notna(row['user_age']) else None,\n",
    "                'signup_date': datetime.now().isoformat()  # Using current time as signup date\n",
    "            }\n",
    "            \n",
    "            # Create the document in Appwrite\n",
    "            result = databases.create_document(\n",
    "                database_id=fireSaleDb['$id'],\n",
    "                collection_id=userProfileCollection['$id'],\n",
    "                document_id=ID.unique(),\n",
    "                data=document_data\n",
    "            )\n",
    "            \n",
    "            success_count += 1\n",
    "            if success_count % 50 == 0:  # Print progress every 50 records\n",
    "                print(f\"Processed {success_count} records...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"Error processing row {index}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nUpload completed with {success_count} successes and {error_count} errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ece2035-0723-4d25-a694-cc62c7e8dbe2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 records...\n",
      "Processed 100 records...\n",
      "Processed 150 records...\n",
      "Processed 200 records...\n",
      "Processed 250 records...\n",
      "Processed 300 records...\n",
      "Processed 350 records...\n",
      "Processed 400 records...\n",
      "Processed 450 records...\n",
      "Processed 500 records...\n",
      "\n",
      "Upload completed with 500 successes and 0 errors.\n"
     ]
    }
   ],
   "source": [
    "# Load your CSV data\n",
    "user_profile_df = pd.read_csv('dataset/synthetic_user_profile_data.csv')\n",
    "\n",
    "# Upload the first 500 rows\n",
    "upload_user_profiles(user_profile_df, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b9e74-e02d-42e9-8e20-93e56b9b4b45",
   "metadata": {},
   "source": [
    "#### Using a chunk of linkedin company data as active consumers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b2404c9a-2f08-4284-bdd0-6451ef8c6a50",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        name  \\\n",
      "0  Be Nijs * Business- & Concept Development   \n",
      "1             Texas Deaf Chamber of Commerce   \n",
      "2                                CellPraxis®   \n",
      "3                        DIAMOND TOOLS PLANT   \n",
      "4                            Thieves Kitchen   \n",
      "\n",
      "                           industries  \\\n",
      "0    Business Consulting and Services   \n",
      "1      Civic and Social Organizations   \n",
      "2              Biotechnology Research   \n",
      "3  Industrial Machinery Manufacturing   \n",
      "4                    Media Production   \n",
      "\n",
      "                                         specialties  \\\n",
      "0  Expansie bedrijfsactiviteiten, Marketing strat...   \n",
      "1  Business Development, Mentoring, Networking, A...   \n",
      "2  Saúde, Ciências da vida, Terapia celular, Medi...   \n",
      "3                                                NaN   \n",
      "4  On Air Promos, Idents, Commercials, B2B Promos...   \n",
      "\n",
      "                                               about organization_type  \n",
      "0  Voor Startups, ondernemers en bedrijven die he...        Self-Owned  \n",
      "1  MISSION STATEMENT: “The mission of this Chambe...         Nonprofit  \n",
      "2  Fundada em 2008, a Cellpraxis® é uma empresa d...    Privately Held  \n",
      "3  Мы применяем самые передовые технологии для пр...       Partnership  \n",
      "4  Thieves Kitchen is a multi-award-winning produ...    Privately Held  \n",
      "\n",
      "Column headers:\n",
      "['name', 'industries', 'specialties', 'about', 'organization_type']\n",
      "Total number of rows: 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "linkedin_comp_df = pd.read_csv('dataset/linkedin_comp_profile.csv')\n",
    "print(linkedin_comp_df.head())\n",
    "print(\"\\nColumn headers:\")\n",
    "print(linkedin_comp_df.columns.tolist())\n",
    "print(\"Total number of rows:\", linkedin_comp_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3f3a0ab2-b117-4475-a2f1-64f0270b7119",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 records...\n",
      "Processed 100 records...\n",
      "Processed 150 records...\n",
      "Processed 200 records...\n",
      "Processed 250 records...\n",
      "Processed 300 records...\n",
      "Processed 350 records...\n",
      "Processed 400 records...\n",
      "Processed 450 records...\n",
      "Processed 500 records...\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "# Read the CSV file\n",
    "linkedin_comp_df = pd.read_csv('dataset/linkedin_comp_profile.csv').head(500)\n",
    "\n",
    "def create_user_profiles_from_companies(companies_df):\n",
    "    # Initialize counters\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for index, row in companies_df.iterrows():\n",
    "        try:\n",
    "            # Create a document for each company\n",
    "            document = databases.create_document(\n",
    "                database_id=fireSaleDb['$id'],\n",
    "                collection_id=userProfileCollection['$id'],\n",
    "                document_id=ID.unique(),\n",
    "                data={\n",
    "                    'user_id': str(uuid.uuid4()),\n",
    "                    'name': row['name'],\n",
    "                    'email': '',  # No email in source data\n",
    "                    'label': 'CONSUMER',  # Using a fixed label for companies\n",
    "                    'age': 50,  # Not applicable for companies\n",
    "                    'signup_date': datetime.now().isoformat(),\n",
    "                    'consumer_type': 'ORGANIZATION_CONSUMER',\n",
    "                    \n",
    "                }\n",
    "            )\n",
    "            success_count += 1\n",
    "            if success_count % 50 == 0:  # Print progress every 50 records\n",
    "                print(f\"Processed {success_count} records...\")\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"Error processing row {index}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# First ensure the collection exists\n",
    "# prepare_user_profile_collection()\n",
    "\n",
    "# Then create the company profiles\n",
    "create_user_profiles_from_companies(linkedin_comp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e13d9-922b-46f6-8ba3-4fc8bb0e99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def upload_company_profiles(df, limit=500):\n",
    "    # Take the first 500 rows\n",
    "    subset = df.head(limit)\n",
    "    \n",
    "    # Prepare the collection if not already prepared\n",
    "    if userProfileCollection is None:\n",
    "        prepare_user_profile_collection()\n",
    "    \n",
    "    # Initialize counters\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Iterate through each row and create document\n",
    "    for index, row in subset.iterrows():\n",
    "        try:\n",
    "            # Map the CSV columns to our collection attributes\n",
    "            document_data = {\n",
    "                'user_id': row['user_guid'],\n",
    "                'name': row['name'],\n",
    "                'email': row['email'],\n",
    "                'label': row['label'],\n",
    "                'age': int(row['user_age']) if pd.notna(row['user_age']) else None,\n",
    "                'signup_date': datetime.now().isoformat()  # Using current time as signup date\n",
    "            }\n",
    "            \n",
    "            # Create the document in Appwrite\n",
    "            result = databases.create_document(\n",
    "                database_id=fireSaleDb['$id'],\n",
    "                collection_id=userProfileCollection['$id'],\n",
    "                document_id=ID.unique(),\n",
    "                data=document_data\n",
    "            )\n",
    "            \n",
    "            success_count += 1\n",
    "            if success_count % 50 == 0:  # Print progress every 50 records\n",
    "                print(f\"Processed {success_count} records...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"Error processing row {index}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nUpload completed with {success_count} successes and {error_count} errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cad4a3-9de5-4a96-8fc5-5ff097da54b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Link user profile to the facebook ad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a8e9d55-6da2-43a8-85f7-707933a01dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            title  \\\n",
      "0   League of Conservation Voters   \n",
      "1               Indivisible Guide   \n",
      "2  International Rescue Committee   \n",
      "3    Covenant House International   \n",
      "4              Planned Parenthood   \n",
      "\n",
      "                                             message  \\\n",
      "0  <p>BREAKING: Trump’s Department of the Interio...   \n",
      "1  <p>The Mueller investigation is over. Special ...   \n",
      "2  <p>Zimbabwe is reeling from the impact of Cycl...   \n",
      "3  <p>What more can you do in the final hours of ...   \n",
      "4  <p>Say it loud, say it proud: Our rights, our ...   \n",
      "\n",
      "                     advertiser  \\\n",
      "0                           NaN   \n",
      "1                           NaN   \n",
      "2                           NaN   \n",
      "3  Covenant House International   \n",
      "4                           NaN   \n",
      "\n",
      "                                            entities  \\\n",
      "0  [{\"entity\": \"Endangered Species Act\", \"entity_...   \n",
      "1  [{\"entity\": \"Americans\", \"entity_type\": \"Group...   \n",
      "2  [{\"entity\": \"Zimbabwe\", \"entity_type\": \"Region\"}]   \n",
      "3                                                 []   \n",
      "4  [{\"entity\": \"Planned Parenthood\", \"entity_type...   \n",
      "\n",
      "                                                page  \n",
      "0                 https://www.facebook.com/LCVoters/  \n",
      "1         https://www.facebook.com/indivisibleguide/  \n",
      "2  https://www.facebook.com/InternationalRescueCo...  \n",
      "3            https://www.facebook.com/CovenantHouse/  \n",
      "4        https://www.facebook.com/PlannedParenthood/  \n",
      "\n",
      "Column headers:\n",
      "['title', 'message', 'advertiser', 'entities', 'page']\n",
      "Total number of rows: 162324\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "facebook_ad_df = pd.read_csv('dataset/synthetic_facebook_ad_data.csv')\n",
    "print(facebook_ad_df.head())\n",
    "print(\"\\nColumn headers:\")\n",
    "print(facebook_ad_df.columns.tolist())\n",
    "print(\"Total number of rows:\", facebook_ad_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "658ad6ae-a48f-4513-ba86-80584843f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from rake_nltk import Rake\n",
    "import pandas as pd\n",
    "\n",
    "def extract_message_text(html_content):\n",
    "    \"\"\"Extract text from HTML message content\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "def extract_keywords(text, num_keywords=3):\n",
    "    \"\"\"Extract keywords from text using RAKE\"\"\"\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    keywords = r.get_ranked_phrases()[:num_keywords]\n",
    "    return keywords\n",
    "\n",
    "def get_random_user_ids(count):\n",
    "    \"\"\"Get random user IDs from userProfileCollection\"\"\"\n",
    "    user_ids = []\n",
    "    try:\n",
    "        # Get list of users (adjust limit as needed)\n",
    "        users = databases.list_documents(\n",
    "            database_id=fireSaleDb['$id'],\n",
    "            collection_id=userProfileCollection['$id']\n",
    "        )\n",
    "        user_ids = [user['user_id'] for user in users['documents']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching user IDs: {str(e)}\")\n",
    "    \n",
    "    # If we couldn't fetch users, generate some dummy IDs (fallback)\n",
    "    if not user_ids:\n",
    "        print(\"Couldn't fetch user ids, so using dummy ones !!!\")\n",
    "        user_ids = [f\"user_{i}\" for i in range(1, 1001)]\n",
    "    \n",
    "    return random.choices(user_ids, k=count)\n",
    "\n",
    "def upload_social_media_data(df, total_records=5000):\n",
    "    \"\"\"Upload random social media data with 1:5 user ratio\"\"\"\n",
    "    # Prepare the collection if not already prepared\n",
    "    if socialMediaCollection is None:\n",
    "        prepare_social_media_collection()\n",
    "    \n",
    "    # Calculate number of users needed (1:5 ratio)\n",
    "    num_users = total_records // 5\n",
    "    user_ids = get_random_user_ids(num_users)\n",
    "    \n",
    "    # Initialize counters\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Process records\n",
    "    for i in range(total_records):\n",
    "        try:\n",
    "            # Pick a random row from the dataframe\n",
    "            random_row = df.sample(n=1).iloc[0]\n",
    "            \n",
    "            # Extract message text from HTML\n",
    "            message_text = extract_message_text(random_row['message'])\n",
    "            \n",
    "            # Extract keywords\n",
    "            keywords = extract_keywords(message_text, random.randint(1, 3))\n",
    "            \n",
    "            # Process entities to extract groups and organizations\n",
    "            entity_list = []\n",
    "            try:\n",
    "                entities = json.loads(random_row['entities'])\n",
    "                for entity_obj in entities:\n",
    "                    if entity_obj['entity_type'] in ['Group', 'Organization']:\n",
    "                        entity_value = entity_obj['entity'].lower().strip()\n",
    "                        if entity_value not in entity_list:  # Avoid duplicates\n",
    "                            entity_list.append(entity_value)\n",
    "            except (json.JSONDecodeError, KeyError, AttributeError) as e:\n",
    "                print(f\"Error processing entities for record {i}: {str(e)}\")\n",
    "                entity_list = [\"facebook_group\"]  # Fallback value\n",
    "            \n",
    "            # Create document data\n",
    "            document_data = {\n",
    "                'guid': ID.unique(),\n",
    "                'user_id': user_ids[i // 5],  # Same user for 5 records\n",
    "                'platform': \"Facebook\",\n",
    "                'post_count': random.randint(1, 100),\n",
    "                'like_count': random.randint(0, 5000),\n",
    "                'groups': json.dumps({\"associated_grps\": entity_list}),\n",
    "                'follower_count': random.randint(0, 10000),\n",
    "                'top_interests': json.dumps({\"keywords\": keywords}),\n",
    "                'last_active': (datetime.now() - timedelta(days=random.randint(0, 30))).isoformat()\n",
    "            }\n",
    "            \n",
    "            # Create the document in Appwrite\n",
    "            result = databases.create_document(\n",
    "                database_id=fireSaleDb['$id'],\n",
    "                collection_id=socialMediaCollection['$id'],\n",
    "                document_id=ID.unique(),\n",
    "                data=document_data\n",
    "            )\n",
    "            \n",
    "            success_count += 1\n",
    "            if success_count % 100 == 0:  # Print progress every 1000 records\n",
    "                print(f\"Processed {success_count} records...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"Error processing record {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nUpload completed with {success_count} successes and {error_count} errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e100b831-914e-4737-9c33-10dff3a3834a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing record 40: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 50: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 55: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 91: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 100 records...\n",
      "Error processing record 124: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 200 records...\n",
      "Error processing record 222: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 305: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 300 records...\n",
      "Error processing record 341: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 356: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 400 records...\n",
      "Error processing record 418: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 447: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 456: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 482: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 501: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 504: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 500 records...\n",
      "Error processing record 577: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 610: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 600 records...\n",
      "Error processing record 701: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 707: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 700 records...\n",
      "Error processing record 753: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 759: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 800 records...\n",
      "Error processing record 878: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 901: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 900 records...\n",
      "Error processing record 923: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 966: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 993: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1013: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1000 records...\n",
      "Error processing record 1117: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1100 records...\n",
      "Error processing record 1209: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1200 records...\n",
      "Error processing record 1263: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1303: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1300 records...\n",
      "Error processing record 1348: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1427: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1400 records...\n",
      "Error processing record 1454: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1511: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1500 records...\n",
      "Error processing record 1569: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1614: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1600 records...\n",
      "Error processing record 1646: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1687: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1717: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1700 records...\n",
      "Error processing record 1761: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1775: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1840: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1800 records...\n",
      "Error processing record 1890: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1907: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1921: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1943: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1900 records...\n",
      "Error processing record 2021: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2036: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2039: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2000 records...\n",
      "Error processing record 2073: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2078: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2098: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2100 records...\n",
      "Error processing record 2227: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2249: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2200 records...\n",
      "Error processing record 2265: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2307: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2320: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2326: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2300 records...\n",
      "Error processing record 2434: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2457: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2400 records...\n",
      "Error processing record 2464: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2515: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2500 records...\n",
      "Processed 2600 records...\n",
      "Error processing record 2673: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2675: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2707: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2712: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2700 records...\n",
      "Error processing record 2821: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2800 records...\n",
      "Error processing record 2896: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2925: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2940: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2900 records...\n",
      "Error processing record 2986: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3027: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3000 records...\n",
      "Error processing record 3129: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3131: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3100 records...\n",
      "Error processing record 3204: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3220: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3243: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3255: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3200 records...\n",
      "Error processing record 3301: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3377: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3300 records...\n",
      "Error processing record 3402: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3424: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3400 records...\n",
      "Processed 3500 records...\n",
      "Error processing record 3617: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3600 records...\n",
      "Processed 3700 records...\n",
      "Error processing record 3825: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3838: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3859: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3870: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3800 records...\n",
      "Error processing record 3892: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3928: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3961: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3900 records...\n",
      "Error processing record 4023: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4029: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4059: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4000 records...\n",
      "Error processing record 4109: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4100 records...\n",
      "Error processing record 4206: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4222: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4228: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4229: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4232: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4236: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4240: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4200 records...\n",
      "Error processing record 4330: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4382: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4300 records...\n",
      "Error processing record 4407: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4424: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4436: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4446: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4450: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4501: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4400 records...\n",
      "Error processing record 4527: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4500 records...\n",
      "Error processing record 4678: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4600 records...\n",
      "Error processing record 4712: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4728: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4745: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4700 records...\n",
      "Error processing record 4837: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4876: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4886: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4888: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4906: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4800 records...\n",
      "Error processing record 4944: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4957: the JSON object must be str, bytes or bytearray, not float\n",
      "\n",
      "Upload completed with 4878 successes and 122 errors.\n"
     ]
    }
   ],
   "source": [
    "# Load your CSV data\n",
    "facebook_ad_df = pd.read_csv('dataset/synthetic_facebook_ad_data.csv')\n",
    "\n",
    "# Upload the data\n",
    "upload_social_media_data(facebook_ad_df, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef145970-ddf9-45b6-93f7-dbc65dd1bcd8",
   "metadata": {},
   "source": [
    "#### Link user profile to the linkedIn data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4671b539-75bb-414f-b8e3-fea118006dca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            current_company_name  \\\n",
      "0                            NaN   \n",
      "1               Gemeente Utrecht   \n",
      "2                   Network Rail   \n",
      "3                      Freelance   \n",
      "4  Mid-Range Computer Group Inc.   \n",
      "\n",
      "                                            position  \\\n",
      "0  Snr Business Analyst at Emploi et Développemen...   \n",
      "1  Communicatieadviseur Corporate & Strategie Gem...   \n",
      "2               Network Data Manager at Network Rail   \n",
      "3                             Architetto (Freelance)   \n",
      "4  Senior Account Executive at Mid-Range Computer...   \n",
      "\n",
      "                                               about  \\\n",
      "0                                                NaN   \n",
      "1  Allround Marketing & Communicatie Adviseur met...   \n",
      "2  Experienced Data Manager with a demonstrated h...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                               posts  \\\n",
      "0  [{\"attribution\":\"Liked by Catherine Fitzpatric...   \n",
      "1  [{\"attribution\":\"Liked by Margot Bon\",\"img\":\"h...   \n",
      "2                                                NaN   \n",
      "3  [{\"attribution\":\"Liked by Giovanna Panarella\",...   \n",
      "4  [{\"attribution\":\"Liked by Steve Latimer\",\"link...   \n",
      "\n",
      "                                              groups  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3  [{\"img\":null,\"subtitle\":\"-\",\"title\":\"Sustainab...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                          experience  \\\n",
      "0                                                NaN   \n",
      "1  [{\"company\":\"Gemeente Utrecht\",\"company_id\":\"g...   \n",
      "2  [{\"company\":\"Network Rail\",\"company_id\":\"netwo...   \n",
      "3  [{\"company\":\"Freelance\",\"company_id\":null,\"loc...   \n",
      "4  [{\"company\":\"Mid-Range Computer Group Inc.\",\"c...   \n",
      "\n",
      "             educations_details  \n",
      "0    Queen's University Belfast  \n",
      "1                           NaN  \n",
      "2          Brighton Polytechnic  \n",
      "3        Università di Camerino  \n",
      "4  St. Michael's College School  \n",
      "\n",
      "Column headers:\n",
      "['current_company_name', 'position', 'about', 'posts', 'groups', 'experience', 'educations_details']\n",
      "Total number of rows: 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "linkedin_user_df = pd.read_csv('dataset/linkedin_user_profile.csv')\n",
    "print(linkedin_user_df.head())\n",
    "print(\"\\nColumn headers:\")\n",
    "print(linkedin_user_df.columns.tolist())\n",
    "print(\"Total number of rows:\", linkedin_user_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ad0c8e6-7772-40a8-9883-e917851dcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def process_linkedin_experience(experience_json):\n",
    "    \"\"\"Process LinkedIn experience data to extract work experience and industries\"\"\"\n",
    "    try:\n",
    "        experiences = json.loads(experience_json)\n",
    "        work_experience = []\n",
    "        industries = []\n",
    "        \n",
    "        for exp in experiences:\n",
    "            # Extract basic work experience info\n",
    "            if 'company' in exp and 'positions' in exp and len(exp['positions']) > 0:\n",
    "                position = exp['positions'][0]  # Take the first position\n",
    "                work_exp = {\n",
    "                    'company': exp.get('company', ''),\n",
    "                    'title': position.get('title', ''),\n",
    "                    'duration_short': position.get('duration_short', '')\n",
    "                }\n",
    "                work_experience.append(work_exp)\n",
    "                \n",
    "                # Extract industry if available\n",
    "                if 'industry' in exp and exp['industry']:\n",
    "                    industries.append(exp['industry'].lower())\n",
    "            \n",
    "        return {\n",
    "            'work_experience': work_experience,\n",
    "            'industries': list(set(industries))\n",
    "        }# Remove duplicates\n",
    "    except (json.JSONDecodeError, TypeError) as e:\n",
    "        print(f\"Error processing experience data: {str(e)}\")\n",
    "        return {\n",
    "            'work_experience': [],\n",
    "            'industries': []\n",
    "        }\n",
    "\n",
    "def upload_linkedin_data(df, total_records=1000):\n",
    "    \"\"\"Upload LinkedIn user profile data to socialMediaCollection\"\"\"\n",
    "    if socialMediaCollection is None:\n",
    "        prepare_social_media_collection()\n",
    "    \n",
    "    # Get all user IDs once for efficiency\n",
    "    try:\n",
    "        users = databases.list_documents(\n",
    "            database_id=fireSaleDb['$id'],\n",
    "            collection_id=userProfileCollection['$id']\n",
    "        )\n",
    "        all_user_ids = [user['user_id'] for user in users['documents']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching user IDs: {str(e)}\")\n",
    "        all_user_ids = [f\"user_{i}\" for i in range(1, 1001)]\n",
    "    \n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for i in range(total_records):\n",
    "        try:\n",
    "            # Pick a random row and user\n",
    "            random_row = df.sample(n=1).iloc[0]\n",
    "            user_id = random.choice(all_user_ids)\n",
    "            \n",
    "            # Process experience data\n",
    "            experience_data = process_linkedin_experience(random_row['experience'])\n",
    "            \n",
    "            # Process groups (assuming similar format to Facebook groups)\n",
    "            try:\n",
    "                groups = json.loads(random_row['groups'])\n",
    "                group_list = [g.lower().strip() for g in groups if isinstance(g, str)]\n",
    "                group_list = list(set(group_list))[:10]  # Limit to 10 unique groups\n",
    "            except:\n",
    "                group_list = [\"linkedin_group\"]  # Fallback\n",
    "            \n",
    "            # Create document data\n",
    "            document_data = {\n",
    "                'guid': ID.unique(),\n",
    "                'user_id': user_id,\n",
    "                'platform': \"LinkedIn\",\n",
    "                'post_count': random.randint(1, 50),  # LinkedIn typically has fewer posts\n",
    "                'like_count': random.randint(0, 500),\n",
    "                'groups': json.dumps({\"associated_grps\": group_list}),\n",
    "                'follower_count': random.randint(0, 5000),  # LinkedIn typically has fewer followers\n",
    "                'top_interests': json.dumps({\"keywords\": experience_data['industries']}),\n",
    "                'work_exp': json.dumps({\"experience\": experience_data['work_experience']}),\n",
    "                'last_active': (datetime.now() - timedelta(days=random.randint(0, 30))).isoformat()\n",
    "            }\n",
    "            \n",
    "            # Create the document in Appwrite\n",
    "            result = databases.create_document(\n",
    "                database_id=fireSaleDb['$id'],\n",
    "                collection_id=socialMediaCollection['$id'],\n",
    "                document_id=ID.unique(),\n",
    "                data=document_data\n",
    "            )\n",
    "            \n",
    "            success_count += 1\n",
    "            if success_count % 20 == 0:\n",
    "                print(f\"Processed {success_count} LinkedIn profiles...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"Error processing record {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nLinkedIn upload completed: {success_count} successes, {error_count} errors\")\n",
    "    return success_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2d00a7d-7574-45b8-b2b2-a592064a0c5c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 20 LinkedIn profiles...\n",
      "Processed 40 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 60 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 80 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 100 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 120 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 140 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 160 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 180 LinkedIn profiles...\n",
      "Processed 200 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 220 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 240 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 260 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 280 LinkedIn profiles...\n",
      "Processed 300 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 320 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 340 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 360 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 380 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 400 LinkedIn profiles...\n",
      "Processed 420 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 440 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 460 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 480 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 500 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 520 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 540 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 560 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 580 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 600 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 620 LinkedIn profiles...\n",
      "Processed 640 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 660 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 680 LinkedIn profiles...\n",
      "Processed 700 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 720 LinkedIn profiles...\n",
      "Processed 740 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 760 LinkedIn profiles...\n",
      "Processed 780 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 800 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 820 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 840 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 860 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 880 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 900 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 920 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 940 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 960 LinkedIn profiles...\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing experience data: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 980 LinkedIn profiles...\n",
      "Processed 1000 LinkedIn profiles...\n",
      "\n",
      "LinkedIn upload completed: 1000 successes, 0 errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your LinkedIn data\n",
    "linkedin_user_df = pd.read_csv('dataset/linkedin_user_profile.csv')\n",
    "\n",
    "# Upload the data (first 1000 records)\n",
    "upload_linkedin_data(linkedin_user_df, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4e9d9-0d74-47a8-bf60-0e6af08855ed",
   "metadata": {},
   "source": [
    "### Social media data for company profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fe5d0-d731-416e-81e4-f7ca2ffe33f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rafxtgt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rafxtgt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rafxtgt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social media collection already exists\n",
      "Processed 20 LinkedIn profiles...\n",
      "Processed 40 LinkedIn profiles...\n",
      "Processed 60 LinkedIn profiles...\n",
      "Processed 80 LinkedIn profiles...\n",
      "Processed 100 LinkedIn profiles...\n",
      "Processed 120 LinkedIn profiles...\n",
      "Processed 140 LinkedIn profiles...\n",
      "Processed 160 LinkedIn profiles...\n",
      "Processed 180 LinkedIn profiles...\n",
      "Processed 200 LinkedIn profiles...\n",
      "Processed 220 LinkedIn profiles...\n",
      "Processed 240 LinkedIn profiles...\n",
      "Processed 260 LinkedIn profiles...\n",
      "Processed 280 LinkedIn profiles...\n",
      "Processed 300 LinkedIn profiles...\n",
      "Processed 320 LinkedIn profiles...\n",
      "Processed 340 LinkedIn profiles...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "import json\n",
    "from appwrite.id import ID\n",
    "from appwrite.query import Query\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLTK components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def extract_keywords(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Tokenize and clean text\n",
    "    tokens = word_tokenize(str(text).lower())\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Get unique keywords\n",
    "    keywords = list(set(tokens))\n",
    "    return keywords[:20]  # Limit to top 20 keywords\n",
    "\n",
    "def get_user_id_by_name(name):\n",
    "    try:\n",
    "        # Search for user in userProfileCollection by name\n",
    "        users = databases.list_documents(\n",
    "            database_id=fireSaleDb['$id'],\n",
    "            collection_id=userProfileCollection['$id'],\n",
    "            queries=[Query.equal('name', name)]\n",
    "        )\n",
    "        if users['total'] > 0:\n",
    "            return users['documents'][0]['user_id']\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding user {name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_social_media_profiles(companies_df, count=500):\n",
    "    # Ensure collection exists\n",
    "    prepare_social_media_collection()\n",
    "    \n",
    "    # Randomly sample rows\n",
    "    sampled_df = companies_df.sample(n=min(count, len(companies_df)))\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    for index, row in sampled_df.iterrows():\n",
    "        try:\n",
    "            # Find matching user_id\n",
    "            user_id = get_user_id_by_name(row['name'])\n",
    "            \n",
    "            # Extract keywords from specialties and about\n",
    "            specialties_keywords = extract_keywords(row.get('specialties', ''))\n",
    "            about_keywords = extract_keywords(row.get('about', ''))\n",
    "            all_keywords = list(set(specialties_keywords + about_keywords))\n",
    "            \n",
    "            # Create social media document\n",
    "            document = databases.create_document(\n",
    "                database_id=fireSaleDb['$id'],\n",
    "                collection_id=socialMediaCollection['$id'],\n",
    "                document_id=ID.unique(),\n",
    "                data={\n",
    "                    'guid': str(uuid.uuid4()),\n",
    "                    'user_id': user_id,\n",
    "                    'platform': 'LinkedIn',\n",
    "                    'post_count': random.randint(10, 1000),\n",
    "                    'like_count': random.randint(100, 10000),\n",
    "                    'groups': json.dumps([]),  # Empty array as string\n",
    "                    'follower_count': random.randint(100, 50000),\n",
    "                    'top_interests': json.dumps({'keywords': all_keywords}),\n",
    "                    'last_active': datetime.now().isoformat()\n",
    "                }\n",
    "            )\n",
    "            success_count += 1\n",
    "            if success_count % 20 == 0:\n",
    "                print(f\"Processed {success_count} LinkedIn profiles...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"Error processing record {i}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Load the data\n",
    "linkedin_comp_df = pd.read_csv('dataset/linkedin_comp_profile.csv')\n",
    "\n",
    "# Create social media profiles for 500 random companies\n",
    "create_social_media_profiles(linkedin_comp_df, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210b966-0c8c-48e9-b089-496d063eb6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c62ccbe-9bc9-468d-aa46-1cf9586db384",
   "metadata": {},
   "source": [
    "### Construct communication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21d3de62-d2c7-44c2-a16c-032586821d6f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n",
      "\n",
      "Column headers:\n",
      "['file', 'message']\n",
      "Total number of rows: 517401\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "email_df = pd.read_csv('dataset/emails.csv')\n",
    "print(email_df.head())\n",
    "print(\"\\nColumn headers:\")\n",
    "print(email_df.columns.tolist())\n",
    "print(\"Total number of rows:\", email_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f17e79f-5f33-4d93-8789-980dbdddfa5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message content from row 8900:\n",
      "----------------------------------------\n",
      "Message-ID: <257230.1075863588526.JavaMail.evans@thyme>\n",
      "Date: Thu, 24 Aug 2000 09:53:00 -0700 (PDT)\n",
      "From: shapp@caiso.com\n",
      "To: 20participants@caiso.com\n",
      "Subject: CAISO Notice Of Initiation of Proceeding and Refund Effective Dat e\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: \"Happ, Susan\" <SHapp@caiso.com>\n",
      "X-To: ISO Market Participants <IMCEAEX-_O=CAISO_OU=CORPORATE_CN=DISTRIBUTION+20LISTS_CN=ISO+20MARKET+20PARTICIPANTS@caiso.com>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Robert_Badeer_Aug2000\\Notes Folders\\All documents\n",
      "X-Origin: Badeer-R\n",
      "X-FileName: rbadeer.nsf\n",
      "\n",
      "Consistent with FERC's order dated August 23, 2000, in Docket Nos.\n",
      "EL00-95-000 and EL00-98-000 (SDG&E Complaint), attached please find the\n",
      "Notice of Initiation of Proceeding and Refund Effective Date.\n",
      "\n",
      " - Refundnotice.doc\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Select a specific row's message (for example, row index 0)\n",
    "row_index = 8900 # Change this to the row index you want to inspect\n",
    "message_content = email_df.loc[row_index, 'message']\n",
    "\n",
    "# Print the message content\n",
    "print(f\"Message content from row {row_index}:\")\n",
    "print(\"----------------------------------------\")\n",
    "print(message_content)\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "927e177c-6358-4048-9655-e85132fa398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import pandas as pd\n",
    "\n",
    "def extract_email_content(raw_message):\n",
    "    \"\"\"Extract the main content from raw email message\"\"\"\n",
    "    try:\n",
    "        # Split headers from body\n",
    "        parts = raw_message.split('\\n\\n', 1)\n",
    "        if len(parts) > 1:\n",
    "            body = parts[1]\n",
    "        else:\n",
    "            body = raw_message\n",
    "        \n",
    "        # Remove quoted text and signatures\n",
    "        lines = []\n",
    "        for line in body.split('\\n'):\n",
    "            if not line.startswith('>') and not line.startswith('--') and not line.strip() == '':\n",
    "                lines.append(line)\n",
    "        \n",
    "        return ' '.join(lines)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting email content: {str(e)}\")\n",
    "        return raw_message\n",
    "\n",
    "def extract_email_keywords(text, max_keywords=10):\n",
    "    \"\"\"Extract keywords from email text using wordcloud\"\"\"\n",
    "    try:\n",
    "        # Add custom stopwords\n",
    "        stopwords = set(STOPWORDS)\n",
    "        stopwords.update(['subject', 'http', 'https', 'com', 'www', 'cc', 'bcc', 'fw', 're'])\n",
    "        \n",
    "        # Generate word frequencies\n",
    "        wordcloud = WordCloud(stopwords=stopwords).generate(text.lower())\n",
    "        words = wordcloud.process_text(text.lower())\n",
    "        \n",
    "        # Get top keywords by frequency\n",
    "        sorted_words = sorted(words.items(), key=lambda x: x[1], reverse=True)\n",
    "        keywords = [w[0] for w in sorted_words[:max_keywords] if len(w[0]) > 2]  # Filter out very short words\n",
    "        \n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting keywords: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def upload_email_data(df, total_records=517401):\n",
    "    \"\"\"Upload email communication data to communicationCollection\"\"\"\n",
    "    # Prepare the collection if not already prepared\n",
    "    if communicationCollection is None:\n",
    "        prepare_communication_collection()\n",
    "    \n",
    "    # Get all user IDs once for efficiency\n",
    "    try:\n",
    "        users = databases.list_documents(\n",
    "            database_id=fireSaleDb['$id'],\n",
    "            collection_id=userProfileCollection['$id']\n",
    "        )\n",
    "        all_user_ids = [user['user_id'] for user in users['documents']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching user IDs: {str(e)}\")\n",
    "        all_user_ids = [f\"user_{i}\" for i in range(1, 1001)]\n",
    "    \n",
    "    # Determine user distribution (1:n where n is between 5-10)\n",
    "    user_distribution = {}\n",
    "    remaining_records = total_records\n",
    "    while remaining_records > 0:\n",
    "        n = random.randint(5, 10)\n",
    "        user_id = random.choice(all_user_ids)\n",
    "        user_distribution[user_id] = min(n, remaining_records)\n",
    "        remaining_records -= n\n",
    "    \n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for user_id, record_count in user_distribution.items():\n",
    "        for _ in range(record_count):\n",
    "            try:\n",
    "                # Pick a random email\n",
    "                random_row = df.sample(n=1).iloc[0]\n",
    "                \n",
    "                # Extract and process email content\n",
    "                email_content = extract_email_content(random_row['message'])\n",
    "                keywords = extract_email_keywords(email_content)\n",
    "                \n",
    "                # Generate random contacted user (different from sender)\n",
    "                contacted_user = random.choice([uid for uid in all_user_ids if uid != user_id])\n",
    "                \n",
    "                # Create document data\n",
    "                document_data = {\n",
    "                    'guid': ID.unique(),\n",
    "                    'user_id': user_id,\n",
    "                    'medium': \"Email\",\n",
    "                    'contacted_user_id': contacted_user,\n",
    "                    'interaction_count': random.randint(1, 5),\n",
    "                    'topics': json.dumps({\"email_topic_keywords\": keywords}),\n",
    "                    'last_contact_date': (datetime.now() - timedelta(days=random.randint(0, 365))).isoformat()\n",
    "                }\n",
    "                \n",
    "                # Create the document in Appwrite\n",
    "                result = databases.create_document(\n",
    "                    database_id=fireSaleDb['$id'],\n",
    "                    collection_id=communicationCollection['$id'],\n",
    "                    document_id=ID.unique(),\n",
    "                    data=document_data\n",
    "                )\n",
    "                \n",
    "                success_count += 1\n",
    "                if success_count % 500 == 0:\n",
    "                    print(f\"Processed {success_count} emails...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"Error processing email record: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nEmail upload completed: {success_count} successes, {error_count} errors\")\n",
    "    return success_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "52dd300a-b149-40d6-9d99-f9b200c6baa8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting keywords: We need at least 1 word to plot a word cloud, got 0.\n",
      "\n",
      "Email upload completed: 196 successes, 0 errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your email data\n",
    "email_df = pd.read_csv('dataset/emails.csv')\n",
    "\n",
    "# Upload the data (first 500,000 records for example)\n",
    "upload_email_data(email_df, 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c8b5c-a56f-4b97-8405-d29658027fe0",
   "metadata": {},
   "source": [
    "### Behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "172a3b13-267b-41ff-b04c-241b0ff90f50",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User_ID  Age  Gender  Total_App_Usage_Hours  Daily_Screen_Time_Hours  \\\n",
      "0        1   56    Male                   2.61                     7.15   \n",
      "1        2   46    Male                   2.13                    13.79   \n",
      "2        3   32  Female                   7.28                     4.50   \n",
      "3        4   25  Female                   1.20                     6.29   \n",
      "4        5   38    Male                   6.31                    12.59   \n",
      "\n",
      "   Number_of_Apps_Used  Social_Media_Usage_Hours  \\\n",
      "0                   24                      4.43   \n",
      "1                   18                      4.67   \n",
      "2                   11                      4.58   \n",
      "3                   21                      3.18   \n",
      "4                   14                      3.15   \n",
      "\n",
      "   Productivity_App_Usage_Hours  Gaming_App_Usage_Hours     Location  \n",
      "0                          0.55                    2.40  Los Angeles  \n",
      "1                          4.42                    2.43      Chicago  \n",
      "2                          1.71                    2.83      Houston  \n",
      "3                          3.42                    4.58      Phoenix  \n",
      "4                          0.13                    4.00     New York  \n",
      "\n",
      "Column headers:\n",
      "['User_ID', 'Age', 'Gender', 'Total_App_Usage_Hours', 'Daily_Screen_Time_Hours', 'Number_of_Apps_Used', 'Social_Media_Usage_Hours', 'Productivity_App_Usage_Hours', 'Gaming_App_Usage_Hours', 'Location']\n",
      "Total number of rows: 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "behavioral_df = pd.read_csv('dataset/mobile_usage_behavioral_analysis.csv')\n",
    "print(behavioral_df.head())\n",
    "print(\"\\nColumn headers:\")\n",
    "print(behavioral_df.columns.tolist())\n",
    "print(\"Total number of rows:\", behavioral_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef993e50-9bdb-412b-b091-d9f7f280c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def upload_behavioral_data(df, total_records=1000):\n",
    "    \"\"\"Upload behavioral data to behaviorMetadataCollection\"\"\"\n",
    "    # Prepare the collection if not already prepared\n",
    "    if behaviorMetadataCollection is None:\n",
    "        prepare_behaviour_metadata_collection()\n",
    "    \n",
    "    # Get all user IDs once for efficiency\n",
    "    try:\n",
    "        users = databases.list_documents(\n",
    "            database_id=fireSaleDb['$id'],\n",
    "            collection_id=userProfileCollection['$id']\n",
    "        )\n",
    "        all_user_ids = [user['user_id'] for user in users['documents']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching user IDs: {str(e)}\")\n",
    "        all_user_ids = [f\"user_{i}\" for i in range(1, 1001)]\n",
    "    \n",
    "    # Determine user distribution (1:n where n is between 5-10)\n",
    "    user_distribution = {}\n",
    "    remaining_records = min(total_records, len(df))\n",
    "    \n",
    "    while remaining_records > 0:\n",
    "        n = random.randint(5, min(10, remaining_records))\n",
    "        user_id = random.choice(all_user_ids)\n",
    "        user_distribution[user_id] = n\n",
    "        remaining_records -= n\n",
    "    \n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Process each user's records\n",
    "    for user_id, record_count in user_distribution.items():\n",
    "        for _ in range(record_count):\n",
    "            try:\n",
    "                # Pick a random behavioral record\n",
    "                random_row = df.sample(n=1).iloc[0]\n",
    "                \n",
    "                # Determine preferred app categories\n",
    "                app_categories = []\n",
    "                if random_row['Social_Media_Usage_Hours'] > 0:\n",
    "                    app_categories.append(\"Social_Media\")\n",
    "                if random_row['Productivity_App_Usage_Hours'] > 0:\n",
    "                    app_categories.append(\"Productivity\")\n",
    "                if random_row['Gaming_App_Usage_Hours'] > 0:\n",
    "                    app_categories.append(\"Gaming\")\n",
    "                \n",
    "                # If no categories found, use a default\n",
    "                if not app_categories:\n",
    "                    app_categories = [\"Social_Media\"]\n",
    "                \n",
    "                # Create document data\n",
    "                document_data = {\n",
    "                    'guid': ID.unique(),\n",
    "                    'user_id': user_id,\n",
    "                    'device_type': random.choice([\"Android\", \"iOS\"]),\n",
    "                    'active_hours': str(random_row['Total_App_Usage_Hours']),\n",
    "                    'average_daily_screen_time': float(random_row['Daily_Screen_Time_Hours']),\n",
    "                    'preferred_app_categories': json.dumps({\"app_categories\": app_categories})\n",
    "                }\n",
    "                \n",
    "                # Create the document in Appwrite\n",
    "                result = databases.create_document(\n",
    "                    database_id=fireSaleDb['$id'],\n",
    "                    collection_id=behaviorMetadataCollection['$id'],\n",
    "                    document_id=ID.unique(),\n",
    "                    data=document_data\n",
    "                )\n",
    "                \n",
    "                success_count += 1\n",
    "                if success_count % 100 == 0:\n",
    "                    print(f\"Processed {success_count} behavioral records...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"Error processing behavioral record: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nBehavioral data upload completed: {success_count} successes, {error_count} errors\")\n",
    "    return success_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3c5d9120-fbef-4174-9d31-17deaf6d4829",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 behavioral records...\n",
      "\n",
      "Behavioral data upload completed: 163 successes, 0 errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your behavioral data\n",
    "behavioral_df = pd.read_csv('dataset/mobile_usage_behavioral_analysis.csv')\n",
    "\n",
    "# Upload the data (first 1000 records)\n",
    "upload_behavioral_data(behavioral_df, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe739cc-3dcb-4607-a1bc-1a0e1465d578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
