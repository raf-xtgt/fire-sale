{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6900a7-e284-47d7-93eb-bc3df3a269c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./FireSaleVenv/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: pandas in ./FireSaleVenv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./FireSaleVenv/lib/python3.11/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./FireSaleVenv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./FireSaleVenv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./FireSaleVenv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./FireSaleVenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: faker in ./FireSaleVenv/lib/python3.11/site-packages (37.1.0)\n",
      "Requirement already satisfied: tzdata in ./FireSaleVenv/lib/python3.11/site-packages (from faker) (2025.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./FireSaleVenv/lib/python3.11/site-packages (4.13.4)\n",
      "Requirement already satisfied: rake-nltk in ./FireSaleVenv/lib/python3.11/site-packages (1.0.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./FireSaleVenv/lib/python3.11/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./FireSaleVenv/lib/python3.11/site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in ./FireSaleVenv/lib/python3.11/site-packages (from rake-nltk) (3.9.1)\n",
      "Requirement already satisfied: click in ./FireSaleVenv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./FireSaleVenv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./FireSaleVenv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./FireSaleVenv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install pandas\n",
    "!pip install faker\n",
    "!pip install beautifulsoup4 rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d377940b-2c35-4bbb-bbe5-0cba2fd47a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "972557a2-990d-4fa3-aebe-3c20549a51bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from appwrite.client import Client\n",
    "from appwrite.services.databases import Databases\n",
    "from appwrite.id import ID\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95900b0c-bb8b-499b-867e-45ccab788760",
   "metadata": {},
   "source": [
    "##### Library for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65fcfb09-1ca9-47fe-b779-8516c694058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/rafxtgt/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0706ef06-bfdf-4d33-bef0-0e617edd7a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<appwrite.client.Client at 0x7f7feffd9290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client()\n",
    "client.set_endpoint(os.getenv('APPWRITE_API_ENDPOINT'))\n",
    "client.set_project(os.getenv('APPWRITE_PROJECT'))\n",
    "client.set_key(os.getenv('APPWRITE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d86d695-e9b6-43b7-89df-954fce230152",
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = Databases(client)\n",
    "database_name = 'fire-sale-db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2831de2-1a61-42ff-9795-de21ab379d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'fire-sale-db' already exists with ID: 6812c6490009447d68d4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try to get the database to see if it exists\n",
    "    database_list = databases.list()\n",
    "    existing_db = next((db for db in database_list['databases'] if db['name'] == database_name), None)\n",
    "    \n",
    "    if existing_db:\n",
    "        fireSaleDb = existing_db\n",
    "        print(f\"Database '{database_name}' already exists with ID: {existing_db['$id']}\")\n",
    "    else:\n",
    "        # Create the database if it doesn't exist\n",
    "        fireSaleDb = databases.create(\n",
    "            database_id=ID.unique(),\n",
    "            name=database_name\n",
    "        )\n",
    "        print(f\"Created new database '{database_name}' with ID: {fireSaleDb['$id']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing database: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014d4de8-34b7-42ee-ba34-93535efe210f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$id': '6812c6490009447d68d4',\n",
       " 'name': 'fire-sale-db',\n",
       " '$createdAt': '2025-05-01T00:54:33.779+00:00',\n",
       " '$updatedAt': '2025-05-01T00:54:33.779+00:00',\n",
       " 'enabled': True,\n",
       " 'policies': [],\n",
       " 'archives': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fireSaleDb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a77aa8c-41cc-45fd-adc4-99920e0d4fea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### User Profile Collectin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a23c2a7-22e3-4539-a282-144c6aef18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "userProfileCollection = None\n",
    "\n",
    "def prepare_user_profile_collection():\n",
    "  global userProfileCollection\n",
    "  try:\n",
    "    collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "    for collection in collections['collections']:\n",
    "        if collection['name'] == 'user-profile':\n",
    "            userProfileCollection = collection\n",
    "            print(\"User profile collection already exists\")\n",
    "            return\n",
    "  except Exception as e:\n",
    "    print(f\"Error checking for existing collection: {e}\")\n",
    "      \n",
    "    \n",
    "  userProfileCollection = databases.create_collection(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=ID.unique(),\n",
    "    name='user-profile'\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='user_id',\n",
    "    size=255,\n",
    "    required=True\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='name',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='email',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='label',\n",
    "    size=255,\n",
    "    required=True\n",
    "  )\n",
    "    \n",
    "  databases.create_integer_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='age',\n",
    "    required=False,\n",
    "    min=18,\n",
    "    max=150\n",
    "  )\n",
    "    \n",
    "  databases.create_datetime_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=userProfileCollection['$id'],\n",
    "    key='signup_date',\n",
    "    required=True\n",
    "  )\n",
    "  print(\"User profile collection created successfully !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca394faa-f840-4e68-a184-6d699fd40af9",
   "metadata": {},
   "source": [
    "#### Social Media Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "853e36e4-733e-482b-b551-5f2179a8704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "socialMediaCollection = None\n",
    "\n",
    "def prepare_social_media_collection():\n",
    "    global socialMediaCollection\n",
    "\n",
    "    try:\n",
    "        collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "        for collection in collections['collections']:\n",
    "            if collection['name'] == 'social-media':\n",
    "                socialMediaCollection = collection\n",
    "                print(\"Social media collection already exists\")\n",
    "                return\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking for existing collection: {e}\")\n",
    "    \n",
    "    # If collection doesn't exist, create it\n",
    "    socialMediaCollection = databases.create_collection(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=ID.unique(),\n",
    "        name='social-media'\n",
    "    )\n",
    "\n",
    "    # Create all the attributes\n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='guid',\n",
    "        size=255,\n",
    "        required=True\n",
    "    )\n",
    "        \n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='user_id',\n",
    "        size=255,\n",
    "        required=False\n",
    "    )\n",
    "\n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='platform',\n",
    "        size=255,\n",
    "        required=False\n",
    "    )\n",
    "\n",
    "    databases.create_integer_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='post_count',\n",
    "        required=False,\n",
    "        min=0\n",
    "    )  \n",
    "\n",
    "    databases.create_integer_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='like_count',\n",
    "        required=False,\n",
    "        min=0\n",
    "    )  \n",
    "\n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='groups',\n",
    "        required=False, \n",
    "        size=131072\n",
    "    )    \n",
    "        \n",
    "    databases.create_integer_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='follower_count',\n",
    "        required=False,\n",
    "        min=0\n",
    "    ) \n",
    "        \n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='top_interests',\n",
    "        required=False, \n",
    "        size=131072\n",
    "    )  \n",
    "        \n",
    "    databases.create_datetime_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key='last_active',\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    print(\"Created new social media collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae12a8d-eea1-4334-8923-0a32b10c07cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Communication Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7701e348-dd3f-42f9-9626-64aedc6819ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "communicationCollection = None\n",
    "\n",
    "def prepare_communication_collection():\n",
    "  global communicationCollection\n",
    "  try:\n",
    "    collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "    for collection in collections['collections']:\n",
    "        if collection['name'] == 'communication':\n",
    "            communicationCollection = collection\n",
    "            print(\"Communication collection already exists\")\n",
    "            return\n",
    "  except Exception as e:\n",
    "    print(f\"Error checking for existing collection: {e}\")\n",
    "    \n",
    "\n",
    "  communicationCollection = databases.create_collection(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=ID.unique(),\n",
    "    name='communication'\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='guid',\n",
    "    size=255,\n",
    "    required=True\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='user_id',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='medium',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='contacted_user_id',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_integer_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='interaction_count',\n",
    "    required=False,\n",
    "    min=0\n",
    "  )  \n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='topics',\n",
    "    required=False, \n",
    "    size=131072\n",
    "  )    \n",
    "    \n",
    "  databases.create_datetime_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=communicationCollection['$id'],\n",
    "    key='last_contact_date',\n",
    "    required=False\n",
    "  )\n",
    "  print(\"Communication collection created successfully !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4bac8-a3b6-49d4-86bc-1a5ff28e3c40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Location Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb51ff7a-5a62-456d-a889-faeae1509230",
   "metadata": {},
   "outputs": [],
   "source": [
    "locationCollection = None\n",
    "\n",
    "def prepare_location_collection():\n",
    "  global locationCollection\n",
    "  try:\n",
    "    collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "    for collection in collections['collections']:\n",
    "        if collection['name'] == 'location':\n",
    "            locationCollection = collection\n",
    "            print(\"Location collection already exists\")\n",
    "            return\n",
    "  except Exception as e:\n",
    "    print(f\"Error checking for existing collection: {e}\")\n",
    "\n",
    "    \n",
    "  locationCollection = databases.create_collection(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=ID.unique(),\n",
    "    name='location'\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='guid',\n",
    "    size=255,\n",
    "    required=True\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='user_id',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='location_name',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "    \n",
    "  databases.create_float_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='latitude',\n",
    "    required=False\n",
    "  )\n",
    "    \n",
    "  databases.create_float_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='longitude',\n",
    "    required=False\n",
    "  )\n",
    "   \n",
    "  databases.create_datetime_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='check_in_time',\n",
    "    required=False\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=locationCollection['$id'],\n",
    "    key='companions',\n",
    "    required=False, \n",
    "    size=131072\n",
    "  )\n",
    "  print(\"Location collection created successfully !\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe623188-538e-4a6c-b31b-e4a9caa573f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Behavioral Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67ea6321-0864-4e75-a0a6-0a1c602dbc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviorMetadataCollection = None\n",
    "\n",
    "def prepare_behaviour_metadata_collection():\n",
    "  global behaviorMetadataCollection\n",
    "  try:\n",
    "    collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "    for collection in collections['collections']:\n",
    "        if collection['name'] == 'behavioral-metadata':\n",
    "            behaviorMetadataCollection = collection\n",
    "            print(\"Behavioral metadata collection already exists\")\n",
    "            return\n",
    "  except Exception as e:\n",
    "    print(f\"Error checking for existing collection: {e}\")\n",
    "\n",
    "  behaviorMetadataCollection = databases.create_collection(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=ID.unique(),\n",
    "    name='behavioral-metadata'\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='guid',\n",
    "    size=255,\n",
    "    required=True\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='user_id',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='device_type',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='active_hours',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "      \n",
    "  databases.create_float_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='average_daily_screen_time',\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=behaviorMetadataCollection['$id'],\n",
    "    key='preferred_app_categories',\n",
    "    required=False, \n",
    "    size=131072\n",
    "  )\n",
    "  print(\"Behavioral metadata collection created successfully !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7537065-93e7-4189-a22c-cb7ec37310c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### InferredProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae43e5ab-1365-4d46-9090-7f52281f4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferredProfileCollection = None\n",
    "\n",
    "def prepare_inferred_profile_collection():\n",
    "  global inferredProfileCollection\n",
    "  try:\n",
    "    collections = databases.list_collections(database_id=fireSaleDb['$id'])\n",
    "    for collection in collections['collections']:\n",
    "        if collection['name'] == 'inferred-profile':\n",
    "            inferredProfileCollection = collection\n",
    "            print(\"Inferred profile collection already exists\")\n",
    "            return\n",
    "  except Exception as e:\n",
    "    print(f\"Error checking for existing collection: {e}\")\n",
    "\n",
    "  inferredProfileCollection = databases.create_collection(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=ID.unique(),\n",
    "    name='inferred-profile'\n",
    "  )\n",
    "    \n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=inferredProfileCollection['$id'],\n",
    "    key='user_id',\n",
    "    size=255,\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "  databases.create_float_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=inferredProfileCollection['$id'],\n",
    "    key='influence_score',\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "    \n",
    "  databases.create_float_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=inferredProfileCollection['$id'],\n",
    "    key='trust_score',\n",
    "    required=False\n",
    "  )\n",
    "\n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=inferredProfileCollection['$id'],\n",
    "    key='hidden_roles',\n",
    "    required=False, \n",
    "    size=131072\n",
    "  )  \n",
    "\n",
    "  databases.create_string_attribute(\n",
    "    database_id=fireSaleDb['$id'],\n",
    "    collection_id=inferredProfileCollection['$id'],\n",
    "    key='hidden_communities',\n",
    "    required=False, \n",
    "    size=131072\n",
    "  )  \n",
    "  print(\"Inferred profile collection created successfully !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86dd8b8-785c-49d3-8b9e-c773d28d34c2",
   "metadata": {},
   "source": [
    "#### Create the database collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a4b575f-e12a-40e2-8675-9fafb310cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_string_attribute(attribute_name: str, size: int = 255, required: bool = False):\n",
    "    global socialMediaCollection\n",
    "    \n",
    "    if not socialMediaCollection:\n",
    "        raise Exception(\"Social media collection not initialized. Call prepare_social_media_collection() first.\")\n",
    "    \n",
    "    # Check if attribute already exists\n",
    "    existing_attributes = databases.list_attributes(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id']\n",
    "    )['attributes']\n",
    "    \n",
    "    if any(attr['key'] == attribute_name for attr in existing_attributes):\n",
    "        print(f\"Attribute '{attribute_name}' already exists\")\n",
    "        return\n",
    "    \n",
    "    # Create new string attribute\n",
    "    databases.create_string_attribute(\n",
    "        database_id=fireSaleDb['$id'],\n",
    "        collection_id=socialMediaCollection['$id'],\n",
    "        key=attribute_name,\n",
    "        size=size,\n",
    "        required=required\n",
    "    )\n",
    "    print(f\"Added new attribute '{attribute_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5e9c364-97fb-40a5-9445-dc473cb544c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new attribute 'work_exp'\n"
     ]
    }
   ],
   "source": [
    "add_new_string_attribute(\"work_exp\", size=131072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "576b7338-6899-489c-9bc7-d59a03f9caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User profile collection already exists\n",
      "Created new social media collection\n",
      "Communication collection already exists\n",
      "Location collection already exists\n",
      "Behavioral metadata collection already exists\n",
      "Inferred profile collection already exists\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  prepare_user_profile_collection()\n",
    "  prepare_social_media_collection()\n",
    "  prepare_communication_collection()\n",
    "  prepare_location_collection()\n",
    "  prepare_behaviour_metadata_collection()\n",
    "  prepare_inferred_profile_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdf01434-be37-4eff-923e-b956b62d92d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = databases.delete(\n",
    "    database_id = '680fc5380017320a8568'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241fa69-fdfb-4729-8a7c-5f391f4771b1",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c427dbf-8f7e-4094-8d08-ca4a77e564f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Create synthetic user profile data csv from the pseudo file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0036ebb-ae3e-4858-b862-0768ed1450ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              user_guid            name  \\\n",
      "0  6c9799d2-dac3-4098-8fd3-1d0f7ae138bb  Jennifer Blair   \n",
      "1  186648a5-0093-46a1-8932-b696aeb8ed09   Alfred Cooper   \n",
      "2  1a3965d4-ccfb-4a8f-827a-8585a7ad1dc1     Debra Cohen   \n",
      "3  c39101b7-5518-4105-9072-0ba3a1abb1ca   Helen Holland   \n",
      "4  16741755-a8b9-4f5e-883b-4372fa0795bd   Monica Hansen   \n",
      "\n",
      "                          email label  user_age  \n",
      "0            troy75@example.org  NODE        26  \n",
      "1          pamela72@example.com  NODE        26  \n",
      "2      villamichael@example.org  NODE        26  \n",
      "3  hendersonstephen@example.com  NODE        26  \n",
      "4           mclarke@example.org  NODE        26  \n",
      "New CSV created with columns: ['user_guid', 'name', 'email', 'label', 'user_age']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "import uuid\n",
    "\n",
    "# Initialize Faker generator\n",
    "fake = Faker()\n",
    "Faker.seed(42)  # For reproducible results\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('dataset/pseudo_facebook.csv')\n",
    "\n",
    "# Calculate current year and user age\n",
    "current_year = datetime.now().year\n",
    "df['user_age'] = current_year - df['dob_year']\n",
    "df['label'] = 'NODE'\n",
    "\n",
    "# Generate unique synthetic data\n",
    "num_rows = len(df)\n",
    "unique_names = set()\n",
    "unique_emails = set()\n",
    "\n",
    "while len(unique_names) < num_rows:\n",
    "    unique_names.add(fake.unique.name())\n",
    "\n",
    "while len(unique_emails) < num_rows:\n",
    "    unique_emails.add(fake.unique.email())\n",
    "\n",
    "# Add the synthetic columns\n",
    "df['name'] = list(unique_names)\n",
    "df['email'] = list(unique_emails)\n",
    "df['user_guid'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "\n",
    "# Verify uniqueness\n",
    "# print(f\"Unique names generated: {len(df['name'].unique()) == len(df)}\")\n",
    "# print(f\"Unique emails generated: {len(df['email'].unique()) == len(df)}\")\n",
    "\n",
    "# Display the DataFrame with new age column\n",
    "\n",
    "selected_columns = ['user_guid', 'name', 'email', 'label', 'user_age']  \n",
    "new_df = df[selected_columns]\n",
    "print(new_df.head())\n",
    "new_df.to_csv('dataset/synthetic_user_profile_data.csv', index=False)\n",
    "\n",
    "print(f\"New CSV created with columns: {selected_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d5d64-a835-4c52-ab09-c95b26d9ba52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Create a smaller file for user interaction from facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e19875f-78f5-4d17-af9c-d51c15abfdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'html', 'political', 'not_political', 'title', 'message', 'thumbnail', 'created_at', 'updated_at', 'lang', 'images', 'impressions', 'political_probability', 'targeting', 'suppressed', 'targets', 'advertiser', 'entities', 'page', 'lower_page', 'targetings', 'paid_for_by', 'targetedness', 'listbuilding_fundraising_proba']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.csv' with your actual file path\n",
    "file_path = 'dataset/fbpac-ads-en-US.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "headers = df.columns.tolist()\n",
    "\n",
    "# Display the first 5 rows with headers\n",
    "df.head()\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ada8b86-223a-45aa-b60e-f0affecf2a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV created with columns: ['title', 'message', 'advertiser', 'entities', 'page']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read original CSV\n",
    "file_path = 'dataset/fbpac-ads-en-US.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select columns you want to keep (adjust names as needed)\n",
    "selected_columns = ['title', 'message', 'advertiser', 'entities', 'page']  \n",
    "\n",
    "# Create new DataFrame with only selected columns\n",
    "new_df = df[selected_columns]\n",
    "\n",
    "# Save to new CSV\n",
    "new_df.to_csv('dataset/compressed_ad_data.csv', index=False)\n",
    "\n",
    "print(f\"New CSV created with columns: {selected_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545c97e-783f-4bd2-971a-2237cd72787e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Twitter interaction data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2c95f9b-32ca-4c69-9710-2b882a476715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  label\n",
      "0  is upset that he can't update his Facebook by ...      0\n",
      "1  @Kenichan I dived many times for the ball. Man...      0\n",
      "2    my whole body feels itchy and like its on fire       0\n",
      "3  @nationwideclass no, it's not behaving at all....      0\n",
      "4                      @Kwesidei not the whole crew       0\n",
      "Total number of rows: 1599999\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read original CSV\n",
    "file_path_ = 'dataset/twitter_dataset_1.csv'\n",
    "\n",
    "# Try different encodings\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='latin1')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Rename columns\n",
    "new_columns = ['label', 'user_id', 'post_date', 'type', 'user_name', 'tweet']\n",
    "if len(df.columns) == len(new_columns):\n",
    "    df.columns = new_columns  # Rename all columns at once\n",
    "else:\n",
    "    print(\"Warning: Column count does not match. Keeping original column names.\")\n",
    "\n",
    "selected_columns = ['tweet', 'label']  \n",
    "tweet_df_1 = df[selected_columns]\n",
    "print(tweet_df_1.head())\n",
    "print(\"Total number of rows:\", tweet_df_1.shape[0])\n",
    "\n",
    "# # Display results\n",
    "# print(\"\\nFirst 5 rows:\")\n",
    "# print(df.head())\n",
    "\n",
    "# print(\"\\nColumn headers:\")\n",
    "# print(df.columns.tolist())\n",
    "\n",
    "# print(\"\\nNumber of columns:\", len(df.columns))\n",
    "# print(\"Total number of rows:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b34f485-33ee-4251-808e-455631fc76d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  label\n",
      "0  just had a real good moment. i missssssssss hi...      0\n",
      "1         is reading manga  http://plurk.com/p/mzp1e      0\n",
      "2  @comeagainjen http://twitpic.com/2y2lx - http:...      0\n",
      "3  @lapcat Need to send 'em to my accountant tomo...      0\n",
      "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder      0\n",
      "Total number of rows: 10314\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read original CSV\n",
    "file_path_d1 = 'dataset/twitter_dataset_2.csv'\n",
    "selected_columns = ['tweet', 'label']  \n",
    "\n",
    "df1 = pd.read_csv(file_path_d1)\n",
    "tweet_df_2 = df1[selected_columns]\n",
    "\n",
    "# print(df1.head())\n",
    "print(tweet_df_2.head())\n",
    "print(\"Total number of rows:\", tweet_df_2.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbc20330-d89c-4abc-94b0-1d739e612eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (1610313, 2)\n",
      "CSV file saved as 'combined_tweets.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenate the two DataFrames vertically (row-wise)\n",
    "combined_df = pd.concat([tweet_df_2, tweet_df_1], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv('dataset/combined_tweets.csv', index=False)  # index=False avoids saving an extra index column\n",
    "\n",
    "print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "print(\"CSV file saved as 'combined_tweets.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873b710-83cc-49ba-b13f-26a1c3b4759f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### LinkedIn User profile and company data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f23cbd4-1ae2-48dc-b814-f10840885e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    timestamp                            id  \\\n",
      "0  2023-01-10            catherinemcilkenny   \n",
      "1  2022-12-17           margot-bon-51a04624   \n",
      "2  2023-05-17            mike-dean-8509a193   \n",
      "3  2022-05-29  giovanna-panarella-99a0a4167   \n",
      "4  2022-12-06         steve-latimer-3364327   \n",
      "\n",
      "                                     name                       city  \\\n",
      "0  Catherine Fitzpatrick (McIlkenny), B.A                     Canada   \n",
      "1                              Margot Bon  The Randstad, Netherlands   \n",
      "2                               Mike Dean    England, United Kingdom   \n",
      "3                      Giovanna Panarella  Avellino, Campania, Italy   \n",
      "4                           Steve Latimer            Ontario, Canada   \n",
      "\n",
      "  country_code region     current_company:company_id  \\\n",
      "0           CA    NaN                            NaN   \n",
      "1           NL     EU               gemeente-utrecht   \n",
      "2           UK    NaN                   network-rail   \n",
      "3           IT     EU                            NaN   \n",
      "4           CA    NaN  mid-range-computer-group-inc.   \n",
      "\n",
      "            current_company:name  \\\n",
      "0                            NaN   \n",
      "1               Gemeente Utrecht   \n",
      "2                   Network Rail   \n",
      "3                      Freelance   \n",
      "4  Mid-Range Computer Group Inc.   \n",
      "\n",
      "                                            position  following  ...  \\\n",
      "0  Snr Business Analyst at Emploi et Développemen...        NaN  ...   \n",
      "1  Communicatieadviseur Corporate & Strategie Gem...        NaN  ...   \n",
      "2               Network Data Manager at Network Rail        NaN  ...   \n",
      "3                             Architetto (Freelance)      500.0  ...   \n",
      "4  Senior Account Executive at Mid-Range Computer...        NaN  ...   \n",
      "\n",
      "                                  people_also_viewed  \\\n",
      "0  [{\"profile_link\":\"https://ca.linkedin.com/in/l...   \n",
      "1  [{\"profile_link\":\"https://nl.linkedin.com/in/j...   \n",
      "2  [{\"profile_link\":\"https://uk.linkedin.com/in/g...   \n",
      "3  [{\"profile_link\":\"https://it.linkedin.com/in/e...   \n",
      "4  [{\"profile_link\":\"https://ca.linkedin.com/in/d...   \n",
      "\n",
      "             educations_details  \\\n",
      "0    Queen's University Belfast   \n",
      "1                           NaN   \n",
      "2          Brighton Polytechnic   \n",
      "3        Università di Camerino   \n",
      "4  St. Michael's College School   \n",
      "\n",
      "                                           education  \\\n",
      "0  [{\"degree\":\"Bachelor of Arts (B.A.) Honours\",\"...   \n",
      "1  [{\"degree\":\"Scrum en Agile werken\",\"end_year\":...   \n",
      "2  [{\"degree\":\"2:2\",\"end_year\":\"1991\",\"field\":\"El...   \n",
      "3  [{\"degree\":\"“Corso di aggiornamento profession...   \n",
      "4  [{\"degree\":\"\",\"end_year\":\"1978\",\"field\":\"\",\"me...   \n",
      "\n",
      "                                              avatar  \\\n",
      "0  https://media.licdn.com/dms/image/C4E03AQEcz_j...   \n",
      "1  https://static.licdn.com/sc/h/244xhbkr7g40x6bs...   \n",
      "2  https://media.licdn.com/dms/image/C4D03AQHLj-Z...   \n",
      "3  https://media-exp1.licdn.com/dms/image/C4D03AQ...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                           languages  \\\n",
      "0                                                NaN   \n",
      "1  [{\"subtitle\":\"Professional working proficiency...   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                      certifications  \\\n",
      "0                                                NaN   \n",
      "1  [{\"meta\":\"Issued Jun 2013\",\"subtitle\":\"Van der...   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4  [{\"meta\":\"Issued Jan 2022 See credential\",\"sub...   \n",
      "\n",
      "                                     recommendations recommendations_count  \\\n",
      "0                                                NaN                   NaN   \n",
      "1  Menno H. Poort “Ik werk al jaren prettig met M...                   2.0   \n",
      "2                                                NaN                   NaN   \n",
      "3                                                NaN                   NaN   \n",
      "4  Blake Reeves “If I was a customer, I would wan...                   1.0   \n",
      "\n",
      "                                volunteer_experience сourses  \n",
      "0                                                NaN     NaN  \n",
      "1  [{\"cause\":\"\",\"duration\":\"Sep 2010 Jul 2020 9 y...     NaN  \n",
      "2                                                NaN     NaN  \n",
      "3  [{\"cause\":\"Arts and Culture\",\"duration\":\"Jan 2...     NaN  \n",
      "4                                                NaN     NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "Total number of rows: 1000\n",
      "\n",
      "Column headers:\n",
      "['timestamp', 'id', 'name', 'city', 'country_code', 'region', 'current_company:company_id', 'current_company:name', 'position', 'following', 'about', 'posts', 'groups', 'current_company', 'experience', 'url', 'people_also_viewed', 'educations_details', 'education', 'avatar', 'languages', 'certifications', 'recommendations', 'recommendations_count', 'volunteer_experience', 'сourses']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read original CSV\n",
    "file_path_d1 = 'dataset/LinkedIn_people_profiles_datasets.csv'\n",
    "\n",
    "df1 = pd.read_csv(file_path_d1)\n",
    "\n",
    "# print(df1.head())\n",
    "print(df1.head())\n",
    "print(\"Total number of rows:\", df1.shape[0])\n",
    "print(\"\\nColumn headers:\")\n",
    "print(df1.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b8956e-7ee2-4f82-adfd-85b261d381dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            current_company_name  \\\n",
      "0                            NaN   \n",
      "1               Gemeente Utrecht   \n",
      "2                   Network Rail   \n",
      "3                      Freelance   \n",
      "4  Mid-Range Computer Group Inc.   \n",
      "\n",
      "                                            position  \\\n",
      "0  Snr Business Analyst at Emploi et Développemen...   \n",
      "1  Communicatieadviseur Corporate & Strategie Gem...   \n",
      "2               Network Data Manager at Network Rail   \n",
      "3                             Architetto (Freelance)   \n",
      "4  Senior Account Executive at Mid-Range Computer...   \n",
      "\n",
      "                                               about  \\\n",
      "0                                                NaN   \n",
      "1  Allround Marketing & Communicatie Adviseur met...   \n",
      "2  Experienced Data Manager with a demonstrated h...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                               posts  \\\n",
      "0  [{\"attribution\":\"Liked by Catherine Fitzpatric...   \n",
      "1  [{\"attribution\":\"Liked by Margot Bon\",\"img\":\"h...   \n",
      "2                                                NaN   \n",
      "3  [{\"attribution\":\"Liked by Giovanna Panarella\",...   \n",
      "4  [{\"attribution\":\"Liked by Steve Latimer\",\"link...   \n",
      "\n",
      "                                              groups  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3  [{\"img\":null,\"subtitle\":\"-\",\"title\":\"Sustainab...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                          experience  \\\n",
      "0                                                NaN   \n",
      "1  [{\"company\":\"Gemeente Utrecht\",\"company_id\":\"g...   \n",
      "2  [{\"company\":\"Network Rail\",\"company_id\":\"netwo...   \n",
      "3  [{\"company\":\"Freelance\",\"company_id\":null,\"loc...   \n",
      "4  [{\"company\":\"Mid-Range Computer Group Inc.\",\"c...   \n",
      "\n",
      "             educations_details  \n",
      "0    Queen's University Belfast  \n",
      "1                           NaN  \n",
      "2          Brighton Polytechnic  \n",
      "3        Università di Camerino  \n",
      "4  St. Michael's College School  \n"
     ]
    }
   ],
   "source": [
    "df1['current_company_name'] = df1['current_company:name']\n",
    "selected_columns = ['current_company_name', 'position', 'about', 'posts', 'groups', 'experience', 'educations_details']\n",
    "linkedIn_df = df1[selected_columns]\n",
    "print(linkedIn_df.head())\n",
    "linkedIn_df.to_csv('dataset/linkedin_user_profile.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be41fd7-87c2-4620-8d80-93e0d0c7fb6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### LinkedIn company profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a1ee0c7-70b3-493e-a0ed-df0f5e833504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 1000\n",
      "\n",
      "Column headers:\n",
      "['timestamp', 'id', 'name', 'country_code', 'locations', 'formatted_locations', 'followers', 'employees_in_linkedin', 'about', 'specialties', 'company_size', 'organization_type', 'industries', 'website', 'crunchbase_url', 'founded', 'company_id', 'employees', 'headquarters', 'image', 'logo', 'similar', 'sphere', 'url', 'type', 'updates', 'slogan', 'affiliated', 'funding', 'stock_info', 'investors']\n",
      "                                        name  \\\n",
      "0  Be Nijs * Business- & Concept Development   \n",
      "1             Texas Deaf Chamber of Commerce   \n",
      "2                                CellPraxis®   \n",
      "3                        DIAMOND TOOLS PLANT   \n",
      "4                            Thieves Kitchen   \n",
      "\n",
      "                           industries  \\\n",
      "0    Business Consulting and Services   \n",
      "1      Civic and Social Organizations   \n",
      "2              Biotechnology Research   \n",
      "3  Industrial Machinery Manufacturing   \n",
      "4                    Media Production   \n",
      "\n",
      "                                         specialties  \\\n",
      "0  Expansie bedrijfsactiviteiten, Marketing strat...   \n",
      "1  Business Development, Mentoring, Networking, A...   \n",
      "2  Saúde, Ciências da vida, Terapia celular, Medi...   \n",
      "3                                                NaN   \n",
      "4  On Air Promos, Idents, Commercials, B2B Promos...   \n",
      "\n",
      "                                               about organization_type  \n",
      "0  Voor Startups, ondernemers en bedrijven die he...        Self-Owned  \n",
      "1  MISSION STATEMENT: “The mission of this Chambe...         Nonprofit  \n",
      "2  Fundada em 2008, a Cellpraxis® é uma empresa d...    Privately Held  \n",
      "3  Мы применяем самые передовые технологии для пр...       Partnership  \n",
      "4  Thieves Kitchen is a multi-award-winning produ...    Privately Held  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read original CSV\n",
    "file_path_d1 = 'dataset/linkedin_company_profile.csv'\n",
    "\n",
    "df1 = pd.read_csv(file_path_d1)\n",
    "\n",
    "# print(df1.head())\n",
    "# print(df1.head())\n",
    "print(\"Total number of rows:\", df1.shape[0])\n",
    "print(\"\\nColumn headers:\")\n",
    "print(df1.columns.tolist())\n",
    "selected_columns = ['name', 'industries', 'specialties', 'about', 'organization_type']\n",
    "linkedIn_comp_df = df1[selected_columns]\n",
    "print(linkedIn_comp_df.head())\n",
    "linkedIn_comp_df.to_csv('dataset/linkedin_comp_profile.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98920e8a-6001-4949-aab3-0b814355143e",
   "metadata": {},
   "source": [
    "### Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85c3c08b-d537-4c07-bcf1-2a507efc5f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              user_guid            name  \\\n",
      "0  6c9799d2-dac3-4098-8fd3-1d0f7ae138bb  Jennifer Blair   \n",
      "1  186648a5-0093-46a1-8932-b696aeb8ed09   Alfred Cooper   \n",
      "2  1a3965d4-ccfb-4a8f-827a-8585a7ad1dc1     Debra Cohen   \n",
      "3  c39101b7-5518-4105-9072-0ba3a1abb1ca   Helen Holland   \n",
      "4  16741755-a8b9-4f5e-883b-4372fa0795bd   Monica Hansen   \n",
      "\n",
      "                          email label  user_age  \n",
      "0            troy75@example.org  NODE        26  \n",
      "1          pamela72@example.com  NODE        26  \n",
      "2      villamichael@example.org  NODE        26  \n",
      "3  hendersonstephen@example.com  NODE        26  \n",
      "4           mclarke@example.org  NODE        26  \n",
      "\n",
      "Column headers:\n",
      "['user_guid', 'name', 'email', 'label', 'user_age']\n",
      "Total number of rows: 99003\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "user_profile_df = pd.read_csv('dataset/synthetic_user_profile_data.csv')\n",
    "print(user_profile_df.head())\n",
    "print(\"\\nColumn headers:\")\n",
    "print(user_profile_df.columns.tolist())\n",
    "print(\"Total number of rows:\", user_profile_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972dd1e-a70f-48b9-814d-8ce73680f09e",
   "metadata": {},
   "source": [
    "#### Push first 500 rows into user profile dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0fa22c4-308c-4788-a200-dc226325d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def upload_user_profiles(df, limit=500):\n",
    "    # Take the first 500 rows\n",
    "    subset = df.head(limit)\n",
    "    \n",
    "    # Prepare the collection if not already prepared\n",
    "    if userProfileCollection is None:\n",
    "        prepare_user_profile_collection()\n",
    "    \n",
    "    # Initialize counters\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Iterate through each row and create document\n",
    "    for index, row in subset.iterrows():\n",
    "        try:\n",
    "            # Map the CSV columns to our collection attributes\n",
    "            document_data = {\n",
    "                'user_id': row['user_guid'],\n",
    "                'name': row['name'],\n",
    "                'email': row['email'],\n",
    "                'label': row['label'],\n",
    "                'age': int(row['user_age']) if pd.notna(row['user_age']) else None,\n",
    "                'signup_date': datetime.now().isoformat()  # Using current time as signup date\n",
    "            }\n",
    "            \n",
    "            # Create the document in Appwrite\n",
    "            result = databases.create_document(\n",
    "                database_id=fireSaleDb['$id'],\n",
    "                collection_id=userProfileCollection['$id'],\n",
    "                document_id=ID.unique(),\n",
    "                data=document_data\n",
    "            )\n",
    "            \n",
    "            success_count += 1\n",
    "            if success_count % 50 == 0:  # Print progress every 50 records\n",
    "                print(f\"Processed {success_count} records...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"Error processing row {index}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nUpload completed with {success_count} successes and {error_count} errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ece2035-0723-4d25-a694-cc62c7e8dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 records...\n",
      "Processed 100 records...\n",
      "Processed 150 records...\n",
      "Processed 200 records...\n",
      "Processed 250 records...\n",
      "Processed 300 records...\n",
      "Processed 350 records...\n",
      "Processed 400 records...\n",
      "Processed 450 records...\n",
      "Processed 500 records...\n",
      "\n",
      "Upload completed with 500 successes and 0 errors.\n"
     ]
    }
   ],
   "source": [
    "# Load your CSV data\n",
    "user_profile_df = pd.read_csv('dataset/synthetic_user_profile_data.csv')\n",
    "\n",
    "# Upload the first 500 rows\n",
    "upload_user_profiles(user_profile_df, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cad4a3-9de5-4a96-8fc5-5ff097da54b0",
   "metadata": {},
   "source": [
    "#### Link user profile to the facebook ad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a8e9d55-6da2-43a8-85f7-707933a01dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            title  \\\n",
      "0   League of Conservation Voters   \n",
      "1               Indivisible Guide   \n",
      "2  International Rescue Committee   \n",
      "3    Covenant House International   \n",
      "4              Planned Parenthood   \n",
      "\n",
      "                                             message  \\\n",
      "0  <p>BREAKING: Trump’s Department of the Interio...   \n",
      "1  <p>The Mueller investigation is over. Special ...   \n",
      "2  <p>Zimbabwe is reeling from the impact of Cycl...   \n",
      "3  <p>What more can you do in the final hours of ...   \n",
      "4  <p>Say it loud, say it proud: Our rights, our ...   \n",
      "\n",
      "                     advertiser  \\\n",
      "0                           NaN   \n",
      "1                           NaN   \n",
      "2                           NaN   \n",
      "3  Covenant House International   \n",
      "4                           NaN   \n",
      "\n",
      "                                            entities  \\\n",
      "0  [{\"entity\": \"Endangered Species Act\", \"entity_...   \n",
      "1  [{\"entity\": \"Americans\", \"entity_type\": \"Group...   \n",
      "2  [{\"entity\": \"Zimbabwe\", \"entity_type\": \"Region\"}]   \n",
      "3                                                 []   \n",
      "4  [{\"entity\": \"Planned Parenthood\", \"entity_type...   \n",
      "\n",
      "                                                page  \n",
      "0                 https://www.facebook.com/LCVoters/  \n",
      "1         https://www.facebook.com/indivisibleguide/  \n",
      "2  https://www.facebook.com/InternationalRescueCo...  \n",
      "3            https://www.facebook.com/CovenantHouse/  \n",
      "4        https://www.facebook.com/PlannedParenthood/  \n",
      "\n",
      "Column headers:\n",
      "['title', 'message', 'advertiser', 'entities', 'page']\n",
      "Total number of rows: 162324\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "facebook_ad_df = pd.read_csv('dataset/synthetic_facebook_ad_data.csv')\n",
    "print(facebook_ad_df.head())\n",
    "print(\"\\nColumn headers:\")\n",
    "print(facebook_ad_df.columns.tolist())\n",
    "print(\"Total number of rows:\", facebook_ad_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "658ad6ae-a48f-4513-ba86-80584843f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from rake_nltk import Rake\n",
    "import pandas as pd\n",
    "\n",
    "def extract_message_text(html_content):\n",
    "    \"\"\"Extract text from HTML message content\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "def extract_keywords(text, num_keywords=3):\n",
    "    \"\"\"Extract keywords from text using RAKE\"\"\"\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    keywords = r.get_ranked_phrases()[:num_keywords]\n",
    "    return keywords\n",
    "\n",
    "def get_random_user_ids(count):\n",
    "    \"\"\"Get random user IDs from userProfileCollection\"\"\"\n",
    "    user_ids = []\n",
    "    try:\n",
    "        # Get list of users (adjust limit as needed)\n",
    "        users = databases.list_documents(\n",
    "            database_id=fireSaleDb['$id'],\n",
    "            collection_id=userProfileCollection['$id']\n",
    "        )\n",
    "        user_ids = [user['user_id'] for user in users['documents']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching user IDs: {str(e)}\")\n",
    "    \n",
    "    # If we couldn't fetch users, generate some dummy IDs (fallback)\n",
    "    if not user_ids:\n",
    "        print(\"Couldn't fetch user ids, so using dummy ones !!!\")\n",
    "        user_ids = [f\"user_{i}\" for i in range(1, 1001)]\n",
    "    \n",
    "    return random.choices(user_ids, k=count)\n",
    "\n",
    "def upload_social_media_data(df, total_records=5000):\n",
    "    \"\"\"Upload random social media data with 1:5 user ratio\"\"\"\n",
    "    # Prepare the collection if not already prepared\n",
    "    if socialMediaCollection is None:\n",
    "        prepare_social_media_collection()\n",
    "    \n",
    "    # Calculate number of users needed (1:5 ratio)\n",
    "    num_users = total_records // 5\n",
    "    user_ids = get_random_user_ids(num_users)\n",
    "    \n",
    "    # Initialize counters\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Process records\n",
    "    for i in range(total_records):\n",
    "        try:\n",
    "            # Pick a random row from the dataframe\n",
    "            random_row = df.sample(n=1).iloc[0]\n",
    "            \n",
    "            # Extract message text from HTML\n",
    "            message_text = extract_message_text(random_row['message'])\n",
    "            \n",
    "            # Extract keywords\n",
    "            keywords = extract_keywords(message_text, random.randint(1, 3))\n",
    "            \n",
    "            # Process entities to extract groups and organizations\n",
    "            entity_list = []\n",
    "            try:\n",
    "                entities = json.loads(random_row['entities'])\n",
    "                for entity_obj in entities:\n",
    "                    if entity_obj['entity_type'] in ['Group', 'Organization']:\n",
    "                        entity_value = entity_obj['entity'].lower().strip()\n",
    "                        if entity_value not in entity_list:  # Avoid duplicates\n",
    "                            entity_list.append(entity_value)\n",
    "            except (json.JSONDecodeError, KeyError, AttributeError) as e:\n",
    "                print(f\"Error processing entities for record {i}: {str(e)}\")\n",
    "                entity_list = [\"facebook_group\"]  # Fallback value\n",
    "            \n",
    "            # Create document data\n",
    "            document_data = {\n",
    "                'guid': ID.unique(),\n",
    "                'user_id': user_ids[i // 5],  # Same user for 5 records\n",
    "                'platform': \"Facebook\",\n",
    "                'post_count': random.randint(1, 100),\n",
    "                'like_count': random.randint(0, 5000),\n",
    "                'groups': json.dumps({\"associated_grps\": entity_list}),\n",
    "                'follower_count': random.randint(0, 10000),\n",
    "                'top_interests': json.dumps({\"keywords\": keywords}),\n",
    "                'last_active': (datetime.now() - timedelta(days=random.randint(0, 30))).isoformat()\n",
    "            }\n",
    "            \n",
    "            # Create the document in Appwrite\n",
    "            result = databases.create_document(\n",
    "                database_id=fireSaleDb['$id'],\n",
    "                collection_id=socialMediaCollection['$id'],\n",
    "                document_id=ID.unique(),\n",
    "                data=document_data\n",
    "            )\n",
    "            \n",
    "            success_count += 1\n",
    "            if success_count % 100 == 0:  # Print progress every 1000 records\n",
    "                print(f\"Processed {success_count} records...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"Error processing record {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nUpload completed with {success_count} successes and {error_count} errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e100b831-914e-4737-9c33-10dff3a3834a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing record 40: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 50: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 55: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 91: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 100 records...\n",
      "Error processing record 124: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 200 records...\n",
      "Error processing record 222: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 305: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 300 records...\n",
      "Error processing record 341: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 356: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 400 records...\n",
      "Error processing record 418: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 447: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 456: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 482: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 501: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 504: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 500 records...\n",
      "Error processing record 577: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 610: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 600 records...\n",
      "Error processing record 701: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 707: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 700 records...\n",
      "Error processing record 753: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 759: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 800 records...\n",
      "Error processing record 878: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 901: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 900 records...\n",
      "Error processing record 923: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 966: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 993: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1013: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1000 records...\n",
      "Error processing record 1117: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1100 records...\n",
      "Error processing record 1209: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1200 records...\n",
      "Error processing record 1263: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1303: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1300 records...\n",
      "Error processing record 1348: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1427: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1400 records...\n",
      "Error processing record 1454: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1511: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1500 records...\n",
      "Error processing record 1569: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1614: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1600 records...\n",
      "Error processing record 1646: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1687: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1717: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1700 records...\n",
      "Error processing record 1761: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1775: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1840: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1800 records...\n",
      "Error processing record 1890: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1907: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1921: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 1943: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 1900 records...\n",
      "Error processing record 2021: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2036: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2039: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2000 records...\n",
      "Error processing record 2073: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2078: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2098: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2100 records...\n",
      "Error processing record 2227: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2249: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2200 records...\n",
      "Error processing record 2265: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2307: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2320: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2326: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2300 records...\n",
      "Error processing record 2434: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2457: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2400 records...\n",
      "Error processing record 2464: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2515: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2500 records...\n",
      "Processed 2600 records...\n",
      "Error processing record 2673: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2675: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2707: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2712: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2700 records...\n",
      "Error processing record 2821: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2800 records...\n",
      "Error processing record 2896: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2925: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 2940: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 2900 records...\n",
      "Error processing record 2986: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3027: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3000 records...\n",
      "Error processing record 3129: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3131: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3100 records...\n",
      "Error processing record 3204: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3220: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3243: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3255: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3200 records...\n",
      "Error processing record 3301: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3377: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3300 records...\n",
      "Error processing record 3402: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3424: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3400 records...\n",
      "Processed 3500 records...\n",
      "Error processing record 3617: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3600 records...\n",
      "Processed 3700 records...\n",
      "Error processing record 3825: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3838: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3859: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3870: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3800 records...\n",
      "Error processing record 3892: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3928: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 3961: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 3900 records...\n",
      "Error processing record 4023: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4029: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4059: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4000 records...\n",
      "Error processing record 4109: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4100 records...\n",
      "Error processing record 4206: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4222: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4228: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4229: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4232: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4236: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4240: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4200 records...\n",
      "Error processing record 4330: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4382: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4300 records...\n",
      "Error processing record 4407: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4424: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4436: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4446: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4450: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4501: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4400 records...\n",
      "Error processing record 4527: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4500 records...\n",
      "Error processing record 4678: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4600 records...\n",
      "Error processing record 4712: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4728: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4745: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4700 records...\n",
      "Error processing record 4837: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4876: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4886: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4888: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4906: the JSON object must be str, bytes or bytearray, not float\n",
      "Processed 4800 records...\n",
      "Error processing record 4944: the JSON object must be str, bytes or bytearray, not float\n",
      "Error processing record 4957: the JSON object must be str, bytes or bytearray, not float\n",
      "\n",
      "Upload completed with 4878 successes and 122 errors.\n"
     ]
    }
   ],
   "source": [
    "# Load your CSV data\n",
    "facebook_ad_df = pd.read_csv('dataset/synthetic_facebook_ad_data.csv')\n",
    "\n",
    "# Upload the data\n",
    "upload_social_media_data(facebook_ad_df, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef145970-ddf9-45b6-93f7-dbc65dd1bcd8",
   "metadata": {},
   "source": [
    "#### Link user profile to the linkedIn data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4671b539-75bb-414f-b8e3-fea118006dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            current_company_name  \\\n",
      "0                            NaN   \n",
      "1               Gemeente Utrecht   \n",
      "2                   Network Rail   \n",
      "3                      Freelance   \n",
      "4  Mid-Range Computer Group Inc.   \n",
      "\n",
      "                                            position  \\\n",
      "0  Snr Business Analyst at Emploi et Développemen...   \n",
      "1  Communicatieadviseur Corporate & Strategie Gem...   \n",
      "2               Network Data Manager at Network Rail   \n",
      "3                             Architetto (Freelance)   \n",
      "4  Senior Account Executive at Mid-Range Computer...   \n",
      "\n",
      "                                               about  \\\n",
      "0                                                NaN   \n",
      "1  Allround Marketing & Communicatie Adviseur met...   \n",
      "2  Experienced Data Manager with a demonstrated h...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                               posts  \\\n",
      "0  [{\"attribution\":\"Liked by Catherine Fitzpatric...   \n",
      "1  [{\"attribution\":\"Liked by Margot Bon\",\"img\":\"h...   \n",
      "2                                                NaN   \n",
      "3  [{\"attribution\":\"Liked by Giovanna Panarella\",...   \n",
      "4  [{\"attribution\":\"Liked by Steve Latimer\",\"link...   \n",
      "\n",
      "                                              groups  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3  [{\"img\":null,\"subtitle\":\"-\",\"title\":\"Sustainab...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                          experience  \\\n",
      "0                                                NaN   \n",
      "1  [{\"company\":\"Gemeente Utrecht\",\"company_id\":\"g...   \n",
      "2  [{\"company\":\"Network Rail\",\"company_id\":\"netwo...   \n",
      "3  [{\"company\":\"Freelance\",\"company_id\":null,\"loc...   \n",
      "4  [{\"company\":\"Mid-Range Computer Group Inc.\",\"c...   \n",
      "\n",
      "             educations_details  \n",
      "0    Queen's University Belfast  \n",
      "1                           NaN  \n",
      "2          Brighton Polytechnic  \n",
      "3        Università di Camerino  \n",
      "4  St. Michael's College School  \n",
      "\n",
      "Column headers:\n",
      "['current_company_name', 'position', 'about', 'posts', 'groups', 'experience', 'educations_details']\n",
      "Total number of rows: 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "linkedin_user_df = pd.read_csv('dataset/linkedin_user_profile.csv')\n",
    "print(linkedin_user_df.head())\n",
    "print(\"\\nColumn headers:\")\n",
    "print(linkedin_user_df.columns.tolist())\n",
    "print(\"Total number of rows:\", linkedin_user_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0c8e6-7772-40a8-9883-e917851dcfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d00a7d-7574-45b8-b2b2-a592064a0c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
