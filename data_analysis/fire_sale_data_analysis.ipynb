{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c24fd5-23a8-45db-800a-88564747f13a",
   "metadata": {},
   "source": [
    "# Dummy social network using synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce3b3515-e49f-4233-81df-f85cbef609d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting gqlalchemy\n",
      "  Downloading gqlalchemy-1.7.0-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m762.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m835.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting faker\n",
      "  Downloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting adlfs<2025.0.0,>=2023.9.0\n",
      "  Downloading adlfs-2024.12.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dacite<2.0.0,>=1.6.0\n",
      "  Downloading dacite-1.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting neo4j<6.0.0,>=4.4.3\n",
      "  Using cached neo4j-5.28.1-py3-none-any.whl (312 kB)\n",
      "Collecting numpy<2.0.0,>=1.24.1\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Collecting psutil<7.0,>=5.9\n",
      "  Using cached psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Collecting pydantic<3.0.0,>=2.3.0\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pymgclient<2.0.0,>=1.3.1\n",
      "  Downloading pymgclient-1.3.1.tar.gz (125 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tzdata in ./dataAnalysis/lib/python3.11/site-packages (from faker) (2025.2)\n",
      "Collecting azure-core<2.0.0,>=1.28.0\n",
      "  Downloading azure_core-1.33.0-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.1/207.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-datalake-store<0.1,>=0.0.53\n",
      "  Downloading azure_datalake_store-0.0.53-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-identity\n",
      "  Downloading azure_identity-1.21.0-py3-none-any.whl (189 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-storage-blob>=12.17.0\n",
      "  Downloading azure_storage_blob-12.25.1-py3-none-any.whl (406 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.0/407.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.12.0\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp>=3.7.0\n",
      "  Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz in ./dataAnalysis/lib/python3.11/site-packages (from neo4j<6.0.0,>=4.4.3->gqlalchemy) (2025.2)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.1\n",
      "  Downloading pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.12.2 in ./dataAnalysis/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.3.0->gqlalchemy) (4.13.2)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from aiohttp>=3.7.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.5/223.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.1/358.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.21.0 in ./dataAnalysis/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in ./dataAnalysis/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (1.17.0)\n",
      "Requirement already satisfied: cffi in ./dataAnalysis/lib/python3.11/site-packages (from azure-datalake-store<0.1,>=0.0.53->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (1.17.1)\n",
      "Collecting msal<2,>=1.16.0\n",
      "  Downloading msal-1.32.3-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=2.1.4\n",
      "  Using cached cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
      "Collecting isodate>=0.6.1\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Collecting msal-extensions>=1.2.0\n",
      "  Downloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pycparser in ./dataAnalysis/lib/python3.11/site-packages (from cffi->azure-datalake-store<0.1,>=0.0.53->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (2.22)\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./dataAnalysis/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./dataAnalysis/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./dataAnalysis/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./dataAnalysis/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (2025.4.26)\n",
      "Installing collected packages: typing-inspection, pymgclient, PyJWT, pydantic-core, psutil, propcache, numpy, networkx, neo4j, multidict, isodate, fsspec, frozenlist, faker, dacite, annotated-types, aiohappyeyeballs, yarl, pydantic, cryptography, azure-core, aiosignal, azure-storage-blob, aiohttp, msal, msal-extensions, azure-datalake-store, azure-identity, adlfs, gqlalchemy\n",
      "\u001b[33m  DEPRECATION: pymgclient is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for pymgclient ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 7.0.0\n",
      "    Uninstalling psutil-7.0.0:\n",
      "      Successfully uninstalled psutil-7.0.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "Successfully installed PyJWT-2.10.1 adlfs-2024.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 azure-core-1.33.0 azure-datalake-store-0.0.53 azure-identity-1.21.0 azure-storage-blob-12.25.1 cryptography-44.0.2 dacite-1.9.2 faker-37.1.0 frozenlist-1.6.0 fsspec-2025.3.2 gqlalchemy-1.7.0 isodate-0.7.2 msal-1.32.3 msal-extensions-1.3.1 multidict-6.4.3 neo4j-5.28.1 networkx-3.4.2 numpy-1.26.4 propcache-0.3.1 psutil-6.1.1 pydantic-2.11.3 pydantic-core-2.33.1 pymgclient-1.3.1 typing-inspection-0.4.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx gqlalchemy faker\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e710fb-0a6a-4eb3-b9ea-ad7c6d377d98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adlfs==2024.12.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2024.12.0)\n",
      "Requirement already satisfied: aiohappyeyeballs==2.6.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiohttp==3.11.18 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.11.18)\n",
      "Requirement already satisfied: aiosignal==1.3.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: annotated-types==0.7.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: anyio==4.9.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (4.9.0)\n",
      "Requirement already satisfied: argon2-cffi==23.1.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (23.1.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (21.2.0)\n",
      "Requirement already satisfied: arrow==1.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: asttokens==3.0.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: async-lru==2.0.5 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (2.0.5)\n",
      "Requirement already satisfied: attrs==25.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (25.3.0)\n",
      "Requirement already satisfied: azure-core==1.33.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.33.0)\n",
      "Requirement already satisfied: azure-datalake-store==0.0.53 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.0.53)\n",
      "Requirement already satisfied: azure-identity==1.21.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (1.21.0)\n",
      "Requirement already satisfied: azure-storage-blob==12.25.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (12.25.1)\n",
      "Requirement already satisfied: babel==2.17.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (2.17.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.13.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (4.13.4)\n",
      "Requirement already satisfied: bleach==6.2.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (6.2.0)\n",
      "Requirement already satisfied: certifi==2025.4.26 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (2025.4.26)\n",
      "Requirement already satisfied: cffi==1.17.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (3.4.1)\n",
      "Requirement already satisfied: comm==0.2.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (0.2.2)\n",
      "Requirement already satisfied: cryptography==44.0.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 24)) (44.0.2)\n",
      "Requirement already satisfied: dacite==1.9.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (1.9.2)\n",
      "Requirement already satisfied: debugpy==1.8.14 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (1.8.14)\n",
      "Requirement already satisfied: decorator==5.2.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (5.2.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (0.7.1)\n",
      "Requirement already satisfied: executing==2.2.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 29)) (2.2.0)\n",
      "Requirement already satisfied: Faker==37.1.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (37.1.0)\n",
      "Requirement already satisfied: fastjsonschema==2.21.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (2.21.1)\n",
      "Requirement already satisfied: fqdn==1.5.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (1.5.1)\n",
      "Requirement already satisfied: frozenlist==1.6.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (1.6.0)\n",
      "Requirement already satisfied: fsspec==2025.3.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (2025.3.2)\n",
      "Requirement already satisfied: GQLAlchemy==1.7.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 35)) (1.7.0)\n",
      "Requirement already satisfied: h11==0.16.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 36)) (0.16.0)\n",
      "Requirement already satisfied: httpcore==1.0.9 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 37)) (1.0.9)\n",
      "Requirement already satisfied: httpx==0.28.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 38)) (0.28.1)\n",
      "Requirement already satisfied: idna==3.10 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 39)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 40)) (6.29.5)\n",
      "Requirement already satisfied: ipython==9.2.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 41)) (9.2.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 42)) (1.1.1)\n",
      "Requirement already satisfied: ipywidgets==8.1.6 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 43)) (8.1.6)\n",
      "Requirement already satisfied: isodate==0.7.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 44)) (0.7.2)\n",
      "Requirement already satisfied: isoduration==20.11.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 45)) (20.11.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 46)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 47)) (3.1.6)\n",
      "Requirement already satisfied: json5==0.12.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 48)) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer==3.0.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 49)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema==4.23.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 50)) (4.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications==2025.4.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 51)) (2025.4.1)\n",
      "Requirement already satisfied: jupyter==1.1.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 52)) (1.1.1)\n",
      "Requirement already satisfied: jupyter-console==6.6.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 53)) (6.6.3)\n",
      "Requirement already satisfied: jupyter-events==0.12.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 54)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-lsp==2.2.5 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 55)) (2.2.5)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 56)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 57)) (5.7.2)\n",
      "Requirement already satisfied: jupyter_server==2.15.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 58)) (2.15.0)\n",
      "Requirement already satisfied: jupyter_server_terminals==0.5.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 59)) (0.5.3)\n",
      "Requirement already satisfied: jupyterlab==4.4.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 60)) (4.4.1)\n",
      "Requirement already satisfied: jupyterlab_pygments==0.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 61)) (0.3.0)\n",
      "Requirement already satisfied: jupyterlab_server==2.27.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 62)) (2.27.3)\n",
      "Requirement already satisfied: jupyterlab_widgets==3.0.14 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 63)) (3.0.14)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 64)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 65)) (0.1.7)\n",
      "Requirement already satisfied: mistune==3.1.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 66)) (3.1.3)\n",
      "Requirement already satisfied: msal==1.32.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 67)) (1.32.3)\n",
      "Requirement already satisfied: msal-extensions==1.3.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 68)) (1.3.1)\n",
      "Requirement already satisfied: multidict==6.4.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 69)) (6.4.3)\n",
      "Requirement already satisfied: nbclient==0.10.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 70)) (0.10.2)\n",
      "Requirement already satisfied: nbconvert==7.16.6 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 71)) (7.16.6)\n",
      "Requirement already satisfied: nbformat==5.10.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 72)) (5.10.4)\n",
      "Requirement already satisfied: neo4j==5.28.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 73)) (5.28.1)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 74)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.4.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 75)) (3.4.2)\n",
      "Requirement already satisfied: notebook==7.4.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 76)) (7.4.1)\n",
      "Requirement already satisfied: notebook_shim==0.2.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 77)) (0.2.4)\n",
      "Requirement already satisfied: numpy==1.26.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 78)) (1.26.4)\n",
      "Requirement already satisfied: overrides==7.7.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 79)) (7.7.0)\n",
      "Requirement already satisfied: packaging==25.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 80)) (25.0)\n",
      "Requirement already satisfied: pandas==2.2.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 81)) (2.2.3)\n",
      "Requirement already satisfied: pandocfilters==1.5.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 82)) (1.5.1)\n",
      "Requirement already satisfied: parso==0.8.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 83)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 84)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.3.7 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 85)) (4.3.7)\n",
      "Requirement already satisfied: prometheus_client==0.21.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 86)) (0.21.1)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 87)) (3.0.51)\n",
      "Requirement already satisfied: propcache==0.3.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 88)) (0.3.1)\n",
      "Requirement already satisfied: psutil==6.1.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 89)) (6.1.1)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 90)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 91)) (0.2.3)\n",
      "Requirement already satisfied: pycparser==2.22 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 92)) (2.22)\n",
      "Requirement already satisfied: pydantic==2.11.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 93)) (2.11.3)\n",
      "Requirement already satisfied: pydantic_core==2.33.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 94)) (2.33.1)\n",
      "Requirement already satisfied: Pygments==2.19.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 95)) (2.19.1)\n",
      "Requirement already satisfied: PyJWT==2.10.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 96)) (2.10.1)\n",
      "Requirement already satisfied: pymgclient==1.3.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 97)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 98)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-json-logger==3.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 99)) (3.3.0)\n",
      "Requirement already satisfied: pytz==2025.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 100)) (2025.2)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 101)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.4.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 102)) (26.4.0)\n",
      "Requirement already satisfied: referencing==0.36.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 103)) (0.36.2)\n",
      "Requirement already satisfied: requests==2.32.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 104)) (2.32.3)\n",
      "Requirement already satisfied: rfc3339-validator==0.1.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 105)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator==0.1.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 106)) (0.1.1)\n",
      "Requirement already satisfied: rpds-py==0.24.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 107)) (0.24.0)\n",
      "Requirement already satisfied: Send2Trash==1.8.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 108)) (1.8.3)\n",
      "Requirement already satisfied: six==1.17.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 109)) (1.17.0)\n",
      "Requirement already satisfied: sniffio==1.3.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 110)) (1.3.1)\n",
      "Requirement already satisfied: soupsieve==2.7 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 111)) (2.7)\n",
      "Requirement already satisfied: stack-data==0.6.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 112)) (0.6.3)\n",
      "Requirement already satisfied: terminado==0.18.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 113)) (0.18.1)\n",
      "Requirement already satisfied: tinycss2==1.4.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 114)) (1.4.0)\n",
      "Requirement already satisfied: tornado==6.4.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 115)) (6.4.2)\n",
      "Requirement already satisfied: traitlets==5.14.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 116)) (5.14.3)\n",
      "Requirement already satisfied: types-python-dateutil==2.9.0.20241206 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 117)) (2.9.0.20241206)\n",
      "Requirement already satisfied: typing-inspection==0.4.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 118)) (0.4.0)\n",
      "Requirement already satisfied: typing_extensions==4.13.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 119)) (4.13.2)\n",
      "Requirement already satisfied: tzdata==2025.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 120)) (2025.2)\n",
      "Requirement already satisfied: uri-template==1.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 121)) (1.3.0)\n",
      "Requirement already satisfied: urllib3==2.4.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 122)) (2.4.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 123)) (0.2.13)\n",
      "Requirement already satisfied: webcolors==24.11.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 124)) (24.11.1)\n",
      "Requirement already satisfied: webencodings==0.5.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 125)) (0.5.1)\n",
      "Requirement already satisfied: websocket-client==1.8.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 126)) (1.8.0)\n",
      "Requirement already satisfied: widgetsnbextension==4.0.14 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 127)) (4.0.14)\n",
      "Requirement already satisfied: yarl==1.20.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 128)) (1.20.0)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in ./dataAnalysis/lib/python3.11/site-packages (from jupyterlab==4.4.1->-r requirements.txt (line 60)) (66.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ff994f-5cc2-48b7-94db-9b4aa4007644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import uuid\n",
    "import faker\n",
    "import networkx as nx\n",
    "from gqlalchemy import Memgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033be020-707c-4e05-8bc4-989a1a6b589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker\n",
    "fake = faker.Faker()\n",
    "\n",
    "# Create NetworkX Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Utility function to create a fake user\n",
    "def create_user(platform):\n",
    "    return {\n",
    "        \"user_id\": str(uuid.uuid4()),\n",
    "        \"platform\": platform,\n",
    "        \"name\": fake.name(),\n",
    "        \"email\": fake.email(),\n",
    "        \"phone\": fake.phone_number(),\n",
    "        \"age\": random.randint(18, 65),\n",
    "        \"gender\": random.choice([\"Male\", \"Female\", \"Other\"]),\n",
    "        \"nationality\": fake.country(),\n",
    "        \"friendList\": [],\n",
    "        \"connectionList\": [],\n",
    "        \"emailList\": [],\n",
    "        \"twitterInteraction\": {\n",
    "            \"post_keywords\": random.sample(\n",
    "                [\"floods\", \"company event\", \"conference\", \"catering\", \"parties\"],\n",
    "                k=random.randint(1, 5)\n",
    "            ),\n",
    "            \"post_sentiment\": [random.choice([0, 1]) for _ in range(4)],\n",
    "            \"time_spent_per_post\": [f\"{random.randint(400, 700)}ms\" for _ in range(4)],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733fa6b2-98c9-4289-9178-abf5143e0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Users\n",
      "User generation complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Users\")\n",
    "# Generate Users\n",
    "users = []\n",
    "\n",
    "for platform in [\"Facebook\", \"Twitter\", \"LinkedIn\"]:\n",
    "    for _ in range(100):\n",
    "        users.append(create_user(platform))\n",
    "\n",
    "# Map user_id to user for quick access\n",
    "user_map = {user[\"user_id\"]: user for user in users}\n",
    "\n",
    "# Now randomly create relationships\n",
    "user_ids = list(user_map.keys())\n",
    "print(\"User generation complete\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802ac94a-e50a-47c0-8220-064ddac8c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create the connection lists\n",
      "Connection list creation complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Create the connection lists\")\n",
    "for user in users:\n",
    "    # Random friends for Facebook\n",
    "    if user[\"platform\"] == \"Facebook\":\n",
    "        friends = random.sample(user_ids, k=random.randint(5, 20))\n",
    "        user[\"friendList\"] = friends\n",
    "    \n",
    "    # Random connections for LinkedIn\n",
    "    if user[\"platform\"] == \"LinkedIn\":\n",
    "        connections = random.sample(user_ids, k=random.randint(5, 20))\n",
    "        user[\"connectionList\"] = connections\n",
    "    \n",
    "    # Random email communications\n",
    "    emails = random.sample(user_ids, k=random.randint(5, 20))\n",
    "    user[\"emailList\"] = emails\n",
    "print(\"Connection list creation complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648b2b92-8e93-49fd-8cd8-f95f663b988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add users to graph\n",
      "Addition of users to graph complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Add users to graph\")\n",
    "# Add nodes and edges to NetworkX Graph\n",
    "for user in users:\n",
    "    G.add_node(user[\"user_id\"], **user)\n",
    "\n",
    "for user in users:\n",
    "    for friend_id in user[\"friendList\"]:\n",
    "        if G.has_node(friend_id):\n",
    "            G.add_edge(user[\"user_id\"], friend_id, relationship=\"FRIEND\")\n",
    "\n",
    "    for connection_id in user[\"connectionList\"]:\n",
    "        if G.has_node(connection_id):\n",
    "            G.add_edge(user[\"user_id\"], connection_id, relationship=\"LINKEDIN_CONNECTION\")\n",
    "\n",
    "    for email_id in user[\"emailList\"]:\n",
    "        if G.has_node(email_id):\n",
    "            G.add_edge(user[\"user_id\"], email_id, relationship=\"EMAIL_CONTACT\")\n",
    "print(\"Addition of users to graph complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a915d9-4a31-4938-8a5f-0271edccbaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump data into memgraph\n",
      "Data pushed to Memgraph successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Dump data into memgraph\")\n",
    "# Connect to Memgraph and push the data\n",
    "memgraph = Memgraph()\n",
    "\n",
    "# Optional: Clear database\n",
    "memgraph.drop_database()\n",
    "\n",
    "# Create Nodes\n",
    "for node_id, data in G.nodes(data=True):\n",
    "    query = f\"\"\"\n",
    "    CREATE (:User {{\n",
    "        user_id: \"{data['user_id']}\",\n",
    "        platform: \"{data['platform']}\",\n",
    "        name: \"{data['name']}\",\n",
    "        email: \"{data['email']}\",\n",
    "        phone: \"{data['phone']}\",\n",
    "        age: {data['age']},\n",
    "        gender: \"{data['gender']}\",\n",
    "        nationality: \"{data['nationality']}\",\n",
    "        twitter_post_keywords: {data['twitterInteraction']['post_keywords']},\n",
    "        twitter_post_sentiment: {data['twitterInteraction']['post_sentiment']},\n",
    "        twitter_time_spent_per_post: {data['twitterInteraction']['time_spent_per_post']}\n",
    "    }})\n",
    "    \"\"\"\n",
    "    memgraph.execute(query)\n",
    "\n",
    "# Create Edges\n",
    "for source, target, data in G.edges(data=True):\n",
    "    relationship = data[\"relationship\"]\n",
    "    query = f\"\"\"\n",
    "    MATCH (a:User {{user_id: \"{source}\"}})\n",
    "    MATCH (b:User {{user_id: \"{target}\"}})\n",
    "    CREATE (a)-[:{relationship}]->(b)\n",
    "    \"\"\"\n",
    "    memgraph.execute(query)\n",
    "\n",
    "print(\"Data pushed to Memgraph successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572da52a-cd42-4885-882a-9a127e983fa0",
   "metadata": {},
   "source": [
    "# Company Sales agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc07e02-eb34-4b2f-8bd9-20bab6015f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0a0a4-fa9d-494a-87ef-a2db7cd0b52e",
   "metadata": {},
   "source": [
    "#### Get company employee data from the memgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aecaf70-bc6b-4553-8550-d916e224c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mg_raw_transport_recv: connection closed by server\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "failed to receive chunk size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Collect and print the results\u001b[39;00m\n\u001b[32m     28\u001b[39m users = []\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplatform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplatform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtwitterInteraction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtwitterInteraction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43musers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/work/fire-sale/data_analysis/dataAnalysis/lib/python3.11/site-packages/gqlalchemy/connection.py:91\u001b[39m, in \u001b[36mMemgraphConnection.execute_and_fetch\u001b[39m\u001b[34m(self, query, parameters)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Executes Cypher query and returns iterator of results.\"\"\"\u001b[39;00m\n\u001b[32m     90\u001b[39m cursor = \u001b[38;5;28mself\u001b[39m._connection.cursor()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     93\u001b[39m     row = cursor.fetchone()\n",
      "\u001b[31mDatabaseError\u001b[39m: failed to receive chunk size"
     ]
    }
   ],
   "source": [
    "from gqlalchemy import Memgraph\n",
    "\n",
    "# Connect to Memgraph (by default it connects to localhost:7687)\n",
    "memgraph = Memgraph()\n",
    "\n",
    "# Query to get nodes with 5 to 10 edges\n",
    "query = \"\"\"\n",
    "MATCH (n)\n",
    "WHERE size([(n)-[]-() | 1]) >= 5 AND size([(n)-[]-() | 1]) <= 10\n",
    "RETURN n.user_id AS user_id,\n",
    "       n.platform AS platform,\n",
    "       n.name AS name,\n",
    "       n.email AS email,\n",
    "       n.phone AS phone,\n",
    "       n.age AS age,\n",
    "       n.gender AS gender,\n",
    "       n.nationality AS nationality,\n",
    "       n.friendList AS friendList,\n",
    "       n.connectionList AS connectionList,\n",
    "       n.emailList AS emailList,\n",
    "       n.twitterInteraction AS twitterInteraction\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "results = memgraph.execute_and_fetch(query)\n",
    "\n",
    "# Collect and print the results\n",
    "users = []\n",
    "for record in results:\n",
    "    user = {\n",
    "        \"user_id\": record[\"user_id\"],\n",
    "        \"platform\": record[\"platform\"],\n",
    "        \"name\": record[\"name\"],\n",
    "        \"email\": record[\"email\"],\n",
    "        \"phone\": record[\"phone\"],\n",
    "        \"age\": record[\"age\"],\n",
    "        \"gender\": record[\"gender\"],\n",
    "        \"nationality\": record[\"nationality\"],\n",
    "        \"friendList\": record[\"friendList\"],\n",
    "        \"connectionList\": record[\"connectionList\"],\n",
    "        \"emailList\": record[\"emailList\"],\n",
    "        \"twitterInteraction\": record[\"twitterInteraction\"],\n",
    "    }\n",
    "    users.append(user)\n",
    "\n",
    "# Optional: Pretty print the first few users\n",
    "from pprint import pprint\n",
    "pprint(users[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e96f37-dd68-46a0-994c-a7fbe34ccbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user model\n",
    "class UserData:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        email: str,\n",
    "        phone: str,\n",
    "        age: int,\n",
    "        gender: str,\n",
    "        nationality: str,\n",
    "        friend_list: List[Dict[str, Any]],\n",
    "        connection_list: List[Dict[str, Any]],\n",
    "        email_list: List[Dict[str, Any]],\n",
    "        twitter_interaction: List[str],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.email = email\n",
    "        self.phone = phone\n",
    "        self.age = age\n",
    "        self.gender = gender\n",
    "        self.nationality = nationality\n",
    "        self.friend_list = friend_list\n",
    "        self.connection_list = connection_list\n",
    "        self.email_list = email_list\n",
    "        self.twitter_interaction = twitter_interaction\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"email\": self.email,\n",
    "            \"phone\": self.phone,\n",
    "            \"age\": self.age,\n",
    "            \"gender\": self.gender,\n",
    "            \"nationality\": self.nationality,\n",
    "            \"friend_list\": self.friend_list,\n",
    "            \"connection_list\": self.connection_list,\n",
    "            \"email_list\": self.email_list,\n",
    "            \"twitter_interaction\": self.twitter_interaction,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aed945-1fd5-4505-9665-0b5779974bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct a consolidated company profile\n",
    "def construct_company_profile(user_data_list: List[UserData]) -> Dict[str, Any]:\n",
    "    company_profile = {\n",
    "        \"company_id\": str(uuid4()),\n",
    "        \"total_users\": len(user_data_list),\n",
    "        \"users\": [],\n",
    "        \"aggregated_data\": {\n",
    "            \"average_age\": 0,\n",
    "            \"gender_distribution\": {},\n",
    "            \"nationality_distribution\": {},\n",
    "            \"common_twitter_interactions\": [],\n",
    "            \"social_network_stats\": {\n",
    "                \"total_friends\": 0,\n",
    "                \"total_connections\": 0,\n",
    "                \"total_email_interactions\": 0,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    total_age = 0\n",
    "    gender_counts = {}\n",
    "    nationality_counts = {}\n",
    "    twitter_interaction_counts = {}\n",
    "    total_friends = 0\n",
    "    total_connections = 0\n",
    "    total_email_interactions = 0\n",
    "\n",
    "    for user_data in user_data_list:\n",
    "        user_dict = user_data.to_dict()\n",
    "        company_profile[\"users\"].append(user_dict)\n",
    "\n",
    "        # Aggregate age\n",
    "        total_age += user_data.age\n",
    "\n",
    "        # Aggregate gender distribution\n",
    "        gender = user_data.gender\n",
    "        gender_counts[gender] = gender_counts.get(gender, 0) + 1\n",
    "\n",
    "        # Aggregate nationality distribution\n",
    "        nationality = user_data.nationality\n",
    "        nationality_counts[nationality] = nationality_counts.get(nationality, 0) + 1\n",
    "\n",
    "        # Aggregate Twitter interactions\n",
    "        for keyword in user_data.twitter_interaction:\n",
    "            twitter_interaction_counts[keyword] = twitter_interaction_counts.get(keyword, 0) + 1\n",
    "\n",
    "        # Aggregate social network stats\n",
    "        total_friends += len(user_data.friend_list)\n",
    "        total_connections += len(user_data.connection_list)\n",
    "        total_email_interactions += len(user_data.email_list)\n",
    "\n",
    "    # Calculate averages and distributions\n",
    "    company_profile[\"aggregated_data\"][\"average_age\"] = total_age / len(user_data_list) if user_data_list else 0\n",
    "    company_profile[\"aggregated_data\"][\"gender_distribution\"] = gender_counts\n",
    "    company_profile[\"aggregated_data\"][\"nationality_distribution\"] = nationality_counts\n",
    "    company_profile[\"aggregated_data\"][\"common_twitter_interactions\"] = sorted(\n",
    "        twitter_interaction_counts.items(), key=lambda x: x[1], reverse=True\n",
    "    )[:10]  # Top 10 keywords\n",
    "\n",
    "    company_profile[\"aggregated_data\"][\"social_network_stats\"][\"total_friends\"] = total_friends\n",
    "    company_profile[\"aggregated_data\"][\"social_network_stats\"][\"total_connections\"] = total_connections\n",
    "    company_profile[\"aggregated_data\"][\"social_network_stats\"][\"total_email_interactions\"] = total_email_interactions\n",
    "\n",
    "    return company_profile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
