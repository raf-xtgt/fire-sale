{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf598c20-65f1-4696-be4e-a918816c2568",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Facebook SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22306f4-6757-4968-9f72-dcfb2af8fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Path to your downloaded file\n",
    "file_path = 'data/facebook.tar.gz'  # adjust if needed\n",
    "\n",
    "# Extract the tar.gz file\n",
    "with tarfile.open(file_path, 'r:gz') as tar:\n",
    "    tar.extractall('facebook_data')  # extracts to a folder called facebook_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71440cd-16a5-4eaa-bf9a-acf5e85845e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['facebook', 'facebook_combined.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List the extracted files\n",
    "extracted_files = os.listdir('facebook_data')\n",
    "print(\"Extracted files:\", extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab08723-b984-4924-9c6d-0d203abb4218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   node1  node2\n",
       "0      0      1\n",
       "1      0      2\n",
       "2      0      3\n",
       "3      0      4\n",
       "4      0      5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load edge list (adjust filename if different)\n",
    "edges = pd.read_csv('facebook_data/facebook_combined.txt', \n",
    "                    sep=' ', \n",
    "                    header=None, \n",
    "                    names=['node1', 'node2'])\n",
    "\n",
    "# Display first few rows\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb3e83-7c14-444e-acb3-ebe6bae0624a",
   "metadata": {},
   "source": [
    "### Reading Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee63397-e303-428a-8cdd-cefd7bc1cb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges (first 5):\n",
      "   node1  node2\n",
      "0    236    186\n",
      "1    122    285\n",
      "2     24    346\n",
      "3    271    304\n",
      "4    176      9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: Load edges for ego network '0'\n",
    "edges = pd.read_csv('facebook_data/facebook/0.edges', sep=' ', header=None, names=['node1', 'node2'])\n",
    "print(\"Edges (first 5):\")\n",
    "print(edges.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d56dcb4-0a64-4b8c-bfc2-503286c51ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node features (first 5):\n",
      "   node_id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
      "0        1       0       0       0       0       0       0       0       0   \n",
      "1        2       0       0       0       0       0       0       0       0   \n",
      "2        3       0       0       0       0       0       0       0       1   \n",
      "3        4       0       0       0       0       0       0       0       0   \n",
      "4        5       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   feat_9  ...  feat_215  feat_216  feat_217  feat_218  feat_219  feat_220  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0         1         0         0   \n",
      "3       0  ...         0         0         0         0         0         0   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   feat_221  feat_222  feat_223  feat_224  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 225 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load node features (rows = nodes, columns = features)\n",
    "feat = pd.read_csv('facebook_data/facebook/0.feat', sep=' ', header=None)\n",
    "feat.columns = ['node_id'] + [f'feat_{i}' for i in range(1, len(feat.columns))]\n",
    "print(\"\\nNode features (first 5):\")\n",
    "print(feat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5d4976-2114-4714-963c-54a744cc4ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature descriptions:\n",
      "  feature_id            feature_description\n",
      "0          0  birthday;anonymized feature 0\n",
      "1          1  birthday;anonymized feature 1\n",
      "2          2  birthday;anonymized feature 2\n",
      "3          3  birthday;anonymized feature 3\n",
      "4          4  birthday;anonymized feature 4\n"
     ]
    }
   ],
   "source": [
    "# Load feature names (metadata for features)\n",
    "with open('facebook_data/facebook/0.featnames', 'r') as f:\n",
    "    featnames = [line.strip().split(' ', 1) for line in f]\n",
    "    featnames = pd.DataFrame(featnames, columns=['feature_id', 'feature_description'])\n",
    "print(\"\\nFeature descriptions:\")\n",
    "print(featnames.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d420fa2e-cbb0-40c1-9e37-9f80f2bbc095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 224\n",
      "Unique descriptions: 224\n",
      "\n",
      "Sample unique descriptions:\n",
      "0                birthday;anonymized feature 0\n",
      "1                birthday;anonymized feature 1\n",
      "2                birthday;anonymized feature 2\n",
      "3                birthday;anonymized feature 3\n",
      "4                birthday;anonymized feature 4\n",
      "5                birthday;anonymized feature 5\n",
      "6                birthday;anonymized feature 6\n",
      "7                birthday;anonymized feature 7\n",
      "8    education;classes;id;anonymized feature 8\n",
      "9    education;classes;id;anonymized feature 9\n",
      "Name: feature_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load feature names and split into ID + description\n",
    "with open('facebook_data/facebook/0.featnames', 'r') as f:\n",
    "    featnames = [line.strip().split(' ', 1) for line in f]\n",
    "    featnames_df = pd.DataFrame(featnames, columns=['feature_id', 'feature_description'])\n",
    "\n",
    "# Extract unique descriptions (drop duplicates)\n",
    "unique_descriptions = featnames_df['feature_description'].drop_duplicates()\n",
    "\n",
    "print(f\"Total features: {len(featnames_df)}\")\n",
    "print(f\"Unique descriptions: {len(unique_descriptions)}\")\n",
    "print(\"\\nSample unique descriptions:\")\n",
    "print(unique_descriptions.head(10))  # Show first 10 unique descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a2aaf6f-293a-401b-93ba-06116598c997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ego node features:\n",
      "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   feat_9  ...  feat_214  feat_215  feat_216  feat_217  feat_218  feat_219  \\\n",
      "0       1  ...         0         0         0         0         0         1   \n",
      "\n",
      "   feat_220  feat_221  feat_222  feat_223  \n",
      "0         0         0         0         0  \n",
      "\n",
      "[1 rows x 224 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load ego node features (same structure as .feat but only for the ego)\n",
    "egofeat = pd.read_csv('facebook_data/facebook/0.egofeat', sep=' ', header=None)\n",
    "egofeat.columns = [f'feat_{i}' for i in range(len(egofeat.columns))]\n",
    "print(\"\\nEgo node features:\")\n",
    "print(egofeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb1314ba-64bd-47a3-9c8c-5fe24cf01d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Circles:\n",
      "{'circle0': ['71', '215', '54', '61', '298', '229', '81', '253', '193', '97', '264', '29', '132', '110', '163', '259', '183', '334', '245', '222'], 'circle1': ['173'], 'circle2': ['155', '99', '327', '140', '116', '147', '144', '150', '270'], 'circle3': ['51', '83', '237'], 'circle4': ['125', '344', '295', '257', '55', '122', '223', '59', '268', '280', '84', '156', '258', '236', '250', '239', '69'], 'circle5': ['23'], 'circle6': ['337', '289', '93', '17', '111', '52', '137', '343', '192', '35', '326', '310', '214', '32', '115', '321', '209', '312', '41', '20'], 'circle7': ['225', '46'], 'circle8': ['282'], 'circle9': ['336', '204', '74', '206', '292', '146', '154', '164', '279', '73'], 'circle10': ['42', '14', '216', '2'], 'circle11': ['324', '265', '54', '161', '298', '76', '165', '199', '203', '13', '66', '113', '97', '252', '313', '238', '158', '240', '331', '332', '134', '218', '118', '235', '311', '151', '308', '212', '70', '211'], 'circle12': ['278'], 'circle13': ['138', '131', '68', '143', '86'], 'circle14': ['175', '227'], 'circle15': ['108', '208', '251', '125', '325', '176', '133', '276', '198', '271', '288', '316', '96', '246', '347', '121', '7', '3', '170', '323', '56', '338', '23', '109', '141', '67', '345', '55', '114', '122', '50', '304', '318', '65', '15', '45', '317', '322', '26', '31', '168', '124', '285', '255', '129', '40', '172', '274', '95', '207', '128', '339', '233', '1', '294', '280', '224', '269', '256', '60', '328', '189', '146', '77', '196', '64', '286', '89', '22', '39', '190', '281', '117', '38', '213', '135', '197', '291', '21', '315', '261', '47', '36', '186', '169', '342', '49', '9', '16', '185', '219', '123', '72', '309', '103', '157', '277', '105', '139', '148', '248', '341', '62', '98', '63', '297', '242', '10', '152', '236', '308', '82', '87', '136', '200', '183', '247', '290', '303', '319', '6', '314', '104', '127', '25', '69', '171', '119', '79', '340', '301', '188', '142'], 'circle16': ['251', '94', '330', '5', '34', '299', '254', '24', '180', '194', '281', '101', '266', '135', '197', '173', '36', '9', '85', '57', '37', '258', '309', '80', '139', '202', '187', '249', '58', '127', '48', '92'], 'circle17': ['90', '52', '172', '126', '294', '179', '145', '105', '210'], 'circle18': ['177'], 'circle19': ['93', '33', '333', '17', '137', '44', '343', '326', '214', '115', '312', '41', '20'], 'circle20': ['244', '282', '262', '293', '220', '174'], 'circle21': ['12'], 'circle22': ['267'], 'circle23': ['28', '149', '162']}\n"
     ]
    }
   ],
   "source": [
    "# Load circles (each line is a circle with its members)\n",
    "with open('facebook_data/facebook/0.circles', 'r') as f:\n",
    "    circles = [line.strip().split() for line in f]\n",
    "    circles = {circle[0]: circle[1:] for circle in circles}  # {circle_name: [nodes]}\n",
    "print(\"\\nCircles:\")\n",
    "print(circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c24fd5-23a8-45db-800a-88564747f13a",
   "metadata": {},
   "source": [
    "#  LDBC Social Network Benchmark - SNB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce3b3515-e49f-4233-81df-f85cbef609d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting gqlalchemy\n",
      "  Downloading gqlalchemy-1.7.0-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m762.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m835.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting faker\n",
      "  Downloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting adlfs<2025.0.0,>=2023.9.0\n",
      "  Downloading adlfs-2024.12.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dacite<2.0.0,>=1.6.0\n",
      "  Downloading dacite-1.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting neo4j<6.0.0,>=4.4.3\n",
      "  Using cached neo4j-5.28.1-py3-none-any.whl (312 kB)\n",
      "Collecting numpy<2.0.0,>=1.24.1\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Collecting psutil<7.0,>=5.9\n",
      "  Using cached psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Collecting pydantic<3.0.0,>=2.3.0\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pymgclient<2.0.0,>=1.3.1\n",
      "  Downloading pymgclient-1.3.1.tar.gz (125 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tzdata in ./dataAnalysis/lib/python3.11/site-packages (from faker) (2025.2)\n",
      "Collecting azure-core<2.0.0,>=1.28.0\n",
      "  Downloading azure_core-1.33.0-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.1/207.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-datalake-store<0.1,>=0.0.53\n",
      "  Downloading azure_datalake_store-0.0.53-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-identity\n",
      "  Downloading azure_identity-1.21.0-py3-none-any.whl (189 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-storage-blob>=12.17.0\n",
      "  Downloading azure_storage_blob-12.25.1-py3-none-any.whl (406 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.0/407.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.12.0\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp>=3.7.0\n",
      "  Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz in ./dataAnalysis/lib/python3.11/site-packages (from neo4j<6.0.0,>=4.4.3->gqlalchemy) (2025.2)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.1\n",
      "  Downloading pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.12.2 in ./dataAnalysis/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.3.0->gqlalchemy) (4.13.2)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from aiohttp>=3.7.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.5/223.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.1/358.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.21.0 in ./dataAnalysis/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in ./dataAnalysis/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (1.17.0)\n",
      "Requirement already satisfied: cffi in ./dataAnalysis/lib/python3.11/site-packages (from azure-datalake-store<0.1,>=0.0.53->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (1.17.1)\n",
      "Collecting msal<2,>=1.16.0\n",
      "  Downloading msal-1.32.3-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=2.1.4\n",
      "  Using cached cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
      "Collecting isodate>=0.6.1\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Collecting msal-extensions>=1.2.0\n",
      "  Downloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pycparser in ./dataAnalysis/lib/python3.11/site-packages (from cffi->azure-datalake-store<0.1,>=0.0.53->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (2.22)\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./dataAnalysis/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./dataAnalysis/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./dataAnalysis/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./dataAnalysis/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs<2025.0.0,>=2023.9.0->gqlalchemy) (2025.4.26)\n",
      "Installing collected packages: typing-inspection, pymgclient, PyJWT, pydantic-core, psutil, propcache, numpy, networkx, neo4j, multidict, isodate, fsspec, frozenlist, faker, dacite, annotated-types, aiohappyeyeballs, yarl, pydantic, cryptography, azure-core, aiosignal, azure-storage-blob, aiohttp, msal, msal-extensions, azure-datalake-store, azure-identity, adlfs, gqlalchemy\n",
      "\u001b[33m  DEPRECATION: pymgclient is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for pymgclient ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 7.0.0\n",
      "    Uninstalling psutil-7.0.0:\n",
      "      Successfully uninstalled psutil-7.0.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "Successfully installed PyJWT-2.10.1 adlfs-2024.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 azure-core-1.33.0 azure-datalake-store-0.0.53 azure-identity-1.21.0 azure-storage-blob-12.25.1 cryptography-44.0.2 dacite-1.9.2 faker-37.1.0 frozenlist-1.6.0 fsspec-2025.3.2 gqlalchemy-1.7.0 isodate-0.7.2 msal-1.32.3 msal-extensions-1.3.1 multidict-6.4.3 neo4j-5.28.1 networkx-3.4.2 numpy-1.26.4 propcache-0.3.1 psutil-6.1.1 pydantic-2.11.3 pydantic-core-2.33.1 pymgclient-1.3.1 typing-inspection-0.4.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx gqlalchemy faker\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e710fb-0a6a-4eb3-b9ea-ad7c6d377d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adlfs==2024.12.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2024.12.0)\n",
      "Requirement already satisfied: aiohappyeyeballs==2.6.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiohttp==3.11.18 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.11.18)\n",
      "Requirement already satisfied: aiosignal==1.3.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: annotated-types==0.7.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: anyio==4.9.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (4.9.0)\n",
      "Requirement already satisfied: argon2-cffi==23.1.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (23.1.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (21.2.0)\n",
      "Requirement already satisfied: arrow==1.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: asttokens==3.0.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: async-lru==2.0.5 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (2.0.5)\n",
      "Requirement already satisfied: attrs==25.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (25.3.0)\n",
      "Requirement already satisfied: azure-core==1.33.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.33.0)\n",
      "Requirement already satisfied: azure-datalake-store==0.0.53 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.0.53)\n",
      "Requirement already satisfied: azure-identity==1.21.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (1.21.0)\n",
      "Requirement already satisfied: azure-storage-blob==12.25.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (12.25.1)\n",
      "Requirement already satisfied: babel==2.17.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (2.17.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.13.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (4.13.4)\n",
      "Requirement already satisfied: bleach==6.2.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (6.2.0)\n",
      "Requirement already satisfied: certifi==2025.4.26 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (2025.4.26)\n",
      "Requirement already satisfied: cffi==1.17.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (3.4.1)\n",
      "Requirement already satisfied: comm==0.2.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (0.2.2)\n",
      "Requirement already satisfied: cryptography==44.0.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 24)) (44.0.2)\n",
      "Requirement already satisfied: dacite==1.9.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (1.9.2)\n",
      "Requirement already satisfied: debugpy==1.8.14 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (1.8.14)\n",
      "Requirement already satisfied: decorator==5.2.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (5.2.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (0.7.1)\n",
      "Requirement already satisfied: executing==2.2.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 29)) (2.2.0)\n",
      "Requirement already satisfied: Faker==37.1.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (37.1.0)\n",
      "Requirement already satisfied: fastjsonschema==2.21.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (2.21.1)\n",
      "Requirement already satisfied: fqdn==1.5.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (1.5.1)\n",
      "Requirement already satisfied: frozenlist==1.6.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (1.6.0)\n",
      "Requirement already satisfied: fsspec==2025.3.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (2025.3.2)\n",
      "Requirement already satisfied: GQLAlchemy==1.7.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 35)) (1.7.0)\n",
      "Requirement already satisfied: h11==0.16.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 36)) (0.16.0)\n",
      "Requirement already satisfied: httpcore==1.0.9 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 37)) (1.0.9)\n",
      "Requirement already satisfied: httpx==0.28.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 38)) (0.28.1)\n",
      "Requirement already satisfied: idna==3.10 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 39)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 40)) (6.29.5)\n",
      "Requirement already satisfied: ipython==9.2.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 41)) (9.2.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 42)) (1.1.1)\n",
      "Requirement already satisfied: ipywidgets==8.1.6 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 43)) (8.1.6)\n",
      "Requirement already satisfied: isodate==0.7.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 44)) (0.7.2)\n",
      "Requirement already satisfied: isoduration==20.11.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 45)) (20.11.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 46)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 47)) (3.1.6)\n",
      "Requirement already satisfied: json5==0.12.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 48)) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer==3.0.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 49)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema==4.23.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 50)) (4.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications==2025.4.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 51)) (2025.4.1)\n",
      "Requirement already satisfied: jupyter==1.1.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 52)) (1.1.1)\n",
      "Requirement already satisfied: jupyter-console==6.6.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 53)) (6.6.3)\n",
      "Requirement already satisfied: jupyter-events==0.12.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 54)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-lsp==2.2.5 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 55)) (2.2.5)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 56)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 57)) (5.7.2)\n",
      "Requirement already satisfied: jupyter_server==2.15.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 58)) (2.15.0)\n",
      "Requirement already satisfied: jupyter_server_terminals==0.5.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 59)) (0.5.3)\n",
      "Requirement already satisfied: jupyterlab==4.4.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 60)) (4.4.1)\n",
      "Requirement already satisfied: jupyterlab_pygments==0.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 61)) (0.3.0)\n",
      "Requirement already satisfied: jupyterlab_server==2.27.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 62)) (2.27.3)\n",
      "Requirement already satisfied: jupyterlab_widgets==3.0.14 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 63)) (3.0.14)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 64)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 65)) (0.1.7)\n",
      "Requirement already satisfied: mistune==3.1.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 66)) (3.1.3)\n",
      "Requirement already satisfied: msal==1.32.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 67)) (1.32.3)\n",
      "Requirement already satisfied: msal-extensions==1.3.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 68)) (1.3.1)\n",
      "Requirement already satisfied: multidict==6.4.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 69)) (6.4.3)\n",
      "Requirement already satisfied: nbclient==0.10.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 70)) (0.10.2)\n",
      "Requirement already satisfied: nbconvert==7.16.6 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 71)) (7.16.6)\n",
      "Requirement already satisfied: nbformat==5.10.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 72)) (5.10.4)\n",
      "Requirement already satisfied: neo4j==5.28.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 73)) (5.28.1)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 74)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.4.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 75)) (3.4.2)\n",
      "Requirement already satisfied: notebook==7.4.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 76)) (7.4.1)\n",
      "Requirement already satisfied: notebook_shim==0.2.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 77)) (0.2.4)\n",
      "Requirement already satisfied: numpy==1.26.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 78)) (1.26.4)\n",
      "Requirement already satisfied: overrides==7.7.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 79)) (7.7.0)\n",
      "Requirement already satisfied: packaging==25.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 80)) (25.0)\n",
      "Requirement already satisfied: pandas==2.2.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 81)) (2.2.3)\n",
      "Requirement already satisfied: pandocfilters==1.5.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 82)) (1.5.1)\n",
      "Requirement already satisfied: parso==0.8.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 83)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 84)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.3.7 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 85)) (4.3.7)\n",
      "Requirement already satisfied: prometheus_client==0.21.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 86)) (0.21.1)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 87)) (3.0.51)\n",
      "Requirement already satisfied: propcache==0.3.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 88)) (0.3.1)\n",
      "Requirement already satisfied: psutil==6.1.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 89)) (6.1.1)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 90)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 91)) (0.2.3)\n",
      "Requirement already satisfied: pycparser==2.22 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 92)) (2.22)\n",
      "Requirement already satisfied: pydantic==2.11.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 93)) (2.11.3)\n",
      "Requirement already satisfied: pydantic_core==2.33.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 94)) (2.33.1)\n",
      "Requirement already satisfied: Pygments==2.19.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 95)) (2.19.1)\n",
      "Requirement already satisfied: PyJWT==2.10.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 96)) (2.10.1)\n",
      "Requirement already satisfied: pymgclient==1.3.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 97)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 98)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-json-logger==3.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 99)) (3.3.0)\n",
      "Requirement already satisfied: pytz==2025.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 100)) (2025.2)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 101)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.4.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 102)) (26.4.0)\n",
      "Requirement already satisfied: referencing==0.36.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 103)) (0.36.2)\n",
      "Requirement already satisfied: requests==2.32.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 104)) (2.32.3)\n",
      "Requirement already satisfied: rfc3339-validator==0.1.4 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 105)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator==0.1.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 106)) (0.1.1)\n",
      "Requirement already satisfied: rpds-py==0.24.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 107)) (0.24.0)\n",
      "Requirement already satisfied: Send2Trash==1.8.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 108)) (1.8.3)\n",
      "Requirement already satisfied: six==1.17.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 109)) (1.17.0)\n",
      "Requirement already satisfied: sniffio==1.3.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 110)) (1.3.1)\n",
      "Requirement already satisfied: soupsieve==2.7 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 111)) (2.7)\n",
      "Requirement already satisfied: stack-data==0.6.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 112)) (0.6.3)\n",
      "Requirement already satisfied: terminado==0.18.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 113)) (0.18.1)\n",
      "Requirement already satisfied: tinycss2==1.4.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 114)) (1.4.0)\n",
      "Requirement already satisfied: tornado==6.4.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 115)) (6.4.2)\n",
      "Requirement already satisfied: traitlets==5.14.3 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 116)) (5.14.3)\n",
      "Requirement already satisfied: types-python-dateutil==2.9.0.20241206 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 117)) (2.9.0.20241206)\n",
      "Requirement already satisfied: typing-inspection==0.4.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 118)) (0.4.0)\n",
      "Requirement already satisfied: typing_extensions==4.13.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 119)) (4.13.2)\n",
      "Requirement already satisfied: tzdata==2025.2 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 120)) (2025.2)\n",
      "Requirement already satisfied: uri-template==1.3.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 121)) (1.3.0)\n",
      "Requirement already satisfied: urllib3==2.4.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 122)) (2.4.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 123)) (0.2.13)\n",
      "Requirement already satisfied: webcolors==24.11.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 124)) (24.11.1)\n",
      "Requirement already satisfied: webencodings==0.5.1 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 125)) (0.5.1)\n",
      "Requirement already satisfied: websocket-client==1.8.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 126)) (1.8.0)\n",
      "Requirement already satisfied: widgetsnbextension==4.0.14 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 127)) (4.0.14)\n",
      "Requirement already satisfied: yarl==1.20.0 in ./dataAnalysis/lib/python3.11/site-packages (from -r requirements.txt (line 128)) (1.20.0)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in ./dataAnalysis/lib/python3.11/site-packages (from jupyterlab==4.4.1->-r requirements.txt (line 60)) (66.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ff994f-5cc2-48b7-94db-9b4aa4007644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import uuid\n",
    "import faker\n",
    "import networkx as nx\n",
    "from gqlalchemy import Memgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033be020-707c-4e05-8bc4-989a1a6b589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker\n",
    "fake = faker.Faker()\n",
    "\n",
    "# Create NetworkX Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Utility function to create a fake user\n",
    "def create_user(platform):\n",
    "    return {\n",
    "        \"user_id\": str(uuid.uuid4()),\n",
    "        \"platform\": platform,\n",
    "        \"name\": fake.name(),\n",
    "        \"email\": fake.email(),\n",
    "        \"phone\": fake.phone_number(),\n",
    "        \"age\": random.randint(18, 65),\n",
    "        \"gender\": random.choice([\"Male\", \"Female\", \"Other\"]),\n",
    "        \"nationality\": fake.country(),\n",
    "        \"friendList\": [],\n",
    "        \"connectionList\": [],\n",
    "        \"emailList\": [],\n",
    "        \"twitterInteraction\": {\n",
    "            \"post_keywords\": random.sample(\n",
    "                [\"floods\", \"company event\", \"conference\", \"catering\", \"parties\"],\n",
    "                k=random.randint(1, 5)\n",
    "            ),\n",
    "            \"post_sentiment\": [random.choice([0, 1]) for _ in range(4)],\n",
    "            \"time_spent_per_post\": [f\"{random.randint(400, 700)}ms\" for _ in range(4)],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733fa6b2-98c9-4289-9178-abf5143e0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Users\n",
      "User generation complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Users\")\n",
    "# Generate Users\n",
    "users = []\n",
    "\n",
    "for platform in [\"Facebook\", \"Twitter\", \"LinkedIn\"]:\n",
    "    for _ in range(100):\n",
    "        users.append(create_user(platform))\n",
    "\n",
    "# Map user_id to user for quick access\n",
    "user_map = {user[\"user_id\"]: user for user in users}\n",
    "\n",
    "# Now randomly create relationships\n",
    "user_ids = list(user_map.keys())\n",
    "print(\"User generation complete\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802ac94a-e50a-47c0-8220-064ddac8c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create the connection lists\n",
      "Connection list creation complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Create the connection lists\")\n",
    "for user in users:\n",
    "    # Random friends for Facebook\n",
    "    if user[\"platform\"] == \"Facebook\":\n",
    "        friends = random.sample(user_ids, k=random.randint(5, 20))\n",
    "        user[\"friendList\"] = friends\n",
    "    \n",
    "    # Random connections for LinkedIn\n",
    "    if user[\"platform\"] == \"LinkedIn\":\n",
    "        connections = random.sample(user_ids, k=random.randint(5, 20))\n",
    "        user[\"connectionList\"] = connections\n",
    "    \n",
    "    # Random email communications\n",
    "    emails = random.sample(user_ids, k=random.randint(5, 20))\n",
    "    user[\"emailList\"] = emails\n",
    "print(\"Connection list creation complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648b2b92-8e93-49fd-8cd8-f95f663b988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add users to graph\n",
      "Addition of users to graph complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Add users to graph\")\n",
    "# Add nodes and edges to NetworkX Graph\n",
    "for user in users:\n",
    "    G.add_node(user[\"user_id\"], **user)\n",
    "\n",
    "for user in users:\n",
    "    for friend_id in user[\"friendList\"]:\n",
    "        if G.has_node(friend_id):\n",
    "            G.add_edge(user[\"user_id\"], friend_id, relationship=\"FRIEND\")\n",
    "\n",
    "    for connection_id in user[\"connectionList\"]:\n",
    "        if G.has_node(connection_id):\n",
    "            G.add_edge(user[\"user_id\"], connection_id, relationship=\"LINKEDIN_CONNECTION\")\n",
    "\n",
    "    for email_id in user[\"emailList\"]:\n",
    "        if G.has_node(email_id):\n",
    "            G.add_edge(user[\"user_id\"], email_id, relationship=\"EMAIL_CONTACT\")\n",
    "print(\"Addition of users to graph complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a915d9-4a31-4938-8a5f-0271edccbaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump data into memgraph\n",
      "Data pushed to Memgraph successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Dump data into memgraph\")\n",
    "# Connect to Memgraph and push the data\n",
    "memgraph = Memgraph()\n",
    "\n",
    "# Optional: Clear database\n",
    "memgraph.drop_database()\n",
    "\n",
    "# Create Nodes\n",
    "for node_id, data in G.nodes(data=True):\n",
    "    query = f\"\"\"\n",
    "    CREATE (:User {{\n",
    "        user_id: \"{data['user_id']}\",\n",
    "        platform: \"{data['platform']}\",\n",
    "        name: \"{data['name']}\",\n",
    "        email: \"{data['email']}\",\n",
    "        phone: \"{data['phone']}\",\n",
    "        age: {data['age']},\n",
    "        gender: \"{data['gender']}\",\n",
    "        nationality: \"{data['nationality']}\",\n",
    "        twitter_post_keywords: {data['twitterInteraction']['post_keywords']},\n",
    "        twitter_post_sentiment: {data['twitterInteraction']['post_sentiment']},\n",
    "        twitter_time_spent_per_post: {data['twitterInteraction']['time_spent_per_post']}\n",
    "    }})\n",
    "    \"\"\"\n",
    "    memgraph.execute(query)\n",
    "\n",
    "# Create Edges\n",
    "for source, target, data in G.edges(data=True):\n",
    "    relationship = data[\"relationship\"]\n",
    "    query = f\"\"\"\n",
    "    MATCH (a:User {{user_id: \"{source}\"}})\n",
    "    MATCH (b:User {{user_id: \"{target}\"}})\n",
    "    CREATE (a)-[:{relationship}]->(b)\n",
    "    \"\"\"\n",
    "    memgraph.execute(query)\n",
    "\n",
    "print(\"Data pushed to Memgraph successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
